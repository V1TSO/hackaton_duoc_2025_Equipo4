{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üè• Gu√≠a Hackathon Salud NHANES - Coach de Bienestar Preventivo\n",
    "## Duoc UC 2025 - 27 horas de desarrollo\n",
    "\n",
    "**Objetivo:** Sistema h√≠brido ML + LLM para predicci√≥n de riesgo cardiometab√≥lico y coaching personalizado\n",
    "\n",
    "### üìã Checklist de Entregables\n",
    "- [ ] Modelo ML con AUROC ‚â• 0.80\n",
    "- [ ] API FastAPI con /predict y /coach\n",
    "- [ ] App en Streamlit/Gradio deployada en HF Spaces\n",
    "- [ ] Validaci√≥n temporal sin fuga de datos\n",
    "- [ ] M√©tricas de fairness por subgrupos\n",
    "- [ ] RAG con citas a /kb local\n",
    "- [ ] PDF descargable del plan\n",
    "- [ ] Presentaci√≥n de 10 min\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ FASE 0: Setup Inicial (30 min - H0 a H0.5)\n",
    "\n",
    "### Instalaci√≥n de dependencias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas==2.0.3 (from -r requirements.txt (line 1))\n",
      "  Using cached pandas-2.0.3.tar.gz (5.3 MB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting numpy==1.24.3 (from -r requirements.txt (line 2))\n",
      "  Using cached numpy-1.24.3.tar.gz (10.9 MB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h\u001b[31mERROR: Exception:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/v1tso/dev/new-hack/.venv/lib/python3.13/site-packages/pip/_internal/cli/base_command.py\", line 107, in _run_wrapper\n",
      "    status = _inner_run()\n",
      "  File \"/Users/v1tso/dev/new-hack/.venv/lib/python3.13/site-packages/pip/_internal/cli/base_command.py\", line 98, in _inner_run\n",
      "    return self.run(options, args)\n",
      "           ~~~~~~~~^^^^^^^^^^^^^^^\n",
      "  File \"/Users/v1tso/dev/new-hack/.venv/lib/python3.13/site-packages/pip/_internal/cli/req_command.py\", line 85, in wrapper\n",
      "    return func(self, options, args)\n",
      "  File \"/Users/v1tso/dev/new-hack/.venv/lib/python3.13/site-packages/pip/_internal/commands/install.py\", line 388, in run\n",
      "    requirement_set = resolver.resolve(\n",
      "        reqs, check_supported_wheels=not options.target_dir\n",
      "    )\n",
      "  File \"/Users/v1tso/dev/new-hack/.venv/lib/python3.13/site-packages/pip/_internal/resolution/resolvelib/resolver.py\", line 99, in resolve\n",
      "    result = self._result = resolver.resolve(\n",
      "                            ~~~~~~~~~~~~~~~~^\n",
      "        collected.requirements, max_rounds=limit_how_complex_resolution_can_be\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"/Users/v1tso/dev/new-hack/.venv/lib/python3.13/site-packages/pip/_vendor/resolvelib/resolvers/resolution.py\", line 601, in resolve\n",
      "    state = resolution.resolve(requirements, max_rounds=max_rounds)\n",
      "  File \"/Users/v1tso/dev/new-hack/.venv/lib/python3.13/site-packages/pip/_vendor/resolvelib/resolvers/resolution.py\", line 434, in resolve\n",
      "    self._add_to_criteria(self.state.criteria, r, parent=None)\n",
      "    ~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/v1tso/dev/new-hack/.venv/lib/python3.13/site-packages/pip/_vendor/resolvelib/resolvers/resolution.py\", line 150, in _add_to_criteria\n",
      "    if not criterion.candidates:\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/v1tso/dev/new-hack/.venv/lib/python3.13/site-packages/pip/_vendor/resolvelib/structs.py\", line 194, in __bool__\n",
      "    return bool(self._sequence)\n",
      "  File \"/Users/v1tso/dev/new-hack/.venv/lib/python3.13/site-packages/pip/_internal/resolution/resolvelib/found_candidates.py\", line 165, in __bool__\n",
      "    self._bool = any(self)\n",
      "                 ~~~^^^^^^\n",
      "  File \"/Users/v1tso/dev/new-hack/.venv/lib/python3.13/site-packages/pip/_internal/resolution/resolvelib/found_candidates.py\", line 149, in <genexpr>\n",
      "    return (c for c in iterator if id(c) not in self._incompatible_ids)\n",
      "                       ^^^^^^^^\n",
      "  File \"/Users/v1tso/dev/new-hack/.venv/lib/python3.13/site-packages/pip/_internal/resolution/resolvelib/found_candidates.py\", line 39, in _iter_built\n",
      "    candidate = func()\n",
      "  File \"/Users/v1tso/dev/new-hack/.venv/lib/python3.13/site-packages/pip/_internal/resolution/resolvelib/factory.py\", line 180, in _make_candidate_from_link\n",
      "    base: BaseCandidate | None = self._make_base_candidate_from_link(\n",
      "                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^\n",
      "        link, template, name, version\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"/Users/v1tso/dev/new-hack/.venv/lib/python3.13/site-packages/pip/_internal/resolution/resolvelib/factory.py\", line 226, in _make_base_candidate_from_link\n",
      "    self._link_candidate_cache[link] = LinkCandidate(\n",
      "                                       ~~~~~~~~~~~~~^\n",
      "        link,\n",
      "        ^^^^^\n",
      "    ...<3 lines>...\n",
      "        version=version,\n",
      "        ^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"/Users/v1tso/dev/new-hack/.venv/lib/python3.13/site-packages/pip/_internal/resolution/resolvelib/candidates.py\", line 318, in __init__\n",
      "    super().__init__(\n",
      "    ~~~~~~~~~~~~~~~~^\n",
      "        link=link,\n",
      "        ^^^^^^^^^^\n",
      "    ...<4 lines>...\n",
      "        version=version,\n",
      "        ^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"/Users/v1tso/dev/new-hack/.venv/lib/python3.13/site-packages/pip/_internal/resolution/resolvelib/candidates.py\", line 161, in __init__\n",
      "    self.dist = self._prepare()\n",
      "                ~~~~~~~~~~~~~^^\n",
      "  File \"/Users/v1tso/dev/new-hack/.venv/lib/python3.13/site-packages/pip/_internal/resolution/resolvelib/candidates.py\", line 238, in _prepare\n",
      "    dist = self._prepare_distribution()\n",
      "  File \"/Users/v1tso/dev/new-hack/.venv/lib/python3.13/site-packages/pip/_internal/resolution/resolvelib/candidates.py\", line 329, in _prepare_distribution\n",
      "    return preparer.prepare_linked_requirement(self._ireq, parallel_builds=True)\n",
      "           ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/v1tso/dev/new-hack/.venv/lib/python3.13/site-packages/pip/_internal/operations/prepare.py\", line 543, in prepare_linked_requirement\n",
      "    return self._prepare_linked_requirement(req, parallel_builds)\n",
      "           ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/v1tso/dev/new-hack/.venv/lib/python3.13/site-packages/pip/_internal/operations/prepare.py\", line 658, in _prepare_linked_requirement\n",
      "    dist = _get_prepared_distribution(\n",
      "        req,\n",
      "    ...<3 lines>...\n",
      "        self.check_build_deps,\n",
      "    )\n",
      "  File \"/Users/v1tso/dev/new-hack/.venv/lib/python3.13/site-packages/pip/_internal/operations/prepare.py\", line 77, in _get_prepared_distribution\n",
      "    abstract_dist.prepare_distribution_metadata(\n",
      "    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^\n",
      "        build_env_installer, build_isolation, check_build_deps\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"/Users/v1tso/dev/new-hack/.venv/lib/python3.13/site-packages/pip/_internal/distributions/sdist.py\", line 55, in prepare_distribution_metadata\n",
      "    self._install_build_reqs(build_env_installer)\n",
      "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/v1tso/dev/new-hack/.venv/lib/python3.13/site-packages/pip/_internal/distributions/sdist.py\", line 132, in _install_build_reqs\n",
      "    build_reqs = self._get_build_requires_wheel()\n",
      "  File \"/Users/v1tso/dev/new-hack/.venv/lib/python3.13/site-packages/pip/_internal/distributions/sdist.py\", line 107, in _get_build_requires_wheel\n",
      "    return backend.get_requires_for_build_wheel()\n",
      "           ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^\n",
      "  File \"/Users/v1tso/dev/new-hack/.venv/lib/python3.13/site-packages/pip/_internal/utils/misc.py\", line 694, in get_requires_for_build_wheel\n",
      "    return super().get_requires_for_build_wheel(config_settings=cs)\n",
      "           ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/v1tso/dev/new-hack/.venv/lib/python3.13/site-packages/pip/_vendor/pyproject_hooks/_impl.py\", line 196, in get_requires_for_build_wheel\n",
      "    return self._call_hook(\n",
      "           ~~~~~~~~~~~~~~~^\n",
      "        \"get_requires_for_build_wheel\", {\"config_settings\": config_settings}\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"/Users/v1tso/dev/new-hack/.venv/lib/python3.13/site-packages/pip/_vendor/pyproject_hooks/_impl.py\", line 402, in _call_hook\n",
      "    raise BackendUnavailable(\n",
      "    ...<4 lines>...\n",
      "    )\n",
      "pip._vendor.pyproject_hooks._impl.BackendUnavailable: Cannot import 'setuptools.build_meta'\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# Instalar dependencias desde requirements.txt\n",
    "# Esto asegura que todas las librer√≠as est√©n en las versiones correctas\n",
    "\n",
    "!pip install -r requirements.txt\n",
    "\n",
    "# Si prefieres instalar individualmente (no recomendado):\n",
    "# !pip install -q pandas numpy scikit-learn xgboost lightgbm \\\n",
    "#     fastapi uvicorn pydantic streamlit gradio \\\n",
    "#     openai shap matplotlib seaborn plotly \\\n",
    "#     reportlab fpdf rank-bm25 python-multipart joblib requests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Imports generales\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mplt\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Imports generales\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "np.random.seed(42)\n",
    "\n",
    "# Configuraci√≥n visual\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"‚úÖ Setup completado\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìÇ Estructura de Datos Esperada\n",
    "\n",
    "**IMPORTANTE:** Antes de ejecutar el c√≥digo, aseg√∫rate de tener los datos NHANES en formato CSV.\n",
    "\n",
    "**Estructura de directorios esperada:**\n",
    "```\n",
    "./data/\n",
    "‚îú‚îÄ‚îÄ DEMO_2015_2016.csv    # Demographics (OBLIGATORIO)\n",
    "‚îú‚îÄ‚îÄ EXAM_2015_2016.csv    # Examination (recomendado)\n",
    "‚îú‚îÄ‚îÄ LAB_2015_2016.csv     # Laboratory (recomendado para labels)\n",
    "‚îú‚îÄ‚îÄ QUEST_2015_2016.csv   # Questionnaire (opcional)\n",
    "‚îú‚îÄ‚îÄ DIET_2015_2016.csv    # Dietary (opcional)\n",
    "‚îú‚îÄ‚îÄ DEMO_2017_2018.csv\n",
    "‚îî‚îÄ‚îÄ ... (y as√≠ para cada ciclo permitido)\n",
    "```\n",
    "\n",
    "**Formato de nombres:**\n",
    "- `DEMO_{CICLO}.csv` donde CICLO = `2015_2016` (con gui√≥n bajo)\n",
    "- `EXAM_{CICLO}.csv`\n",
    "- `LAB_{CICLO}.csv`\n",
    "- `QUEST_{CICLO}.csv`\n",
    "- `DIET_{CICLO}.csv`\n",
    "\n",
    "**Ciclos soportados:**\n",
    "- Entrenamiento: `2015-2016`\n",
    "- Test: `2017-2018`\n",
    "\n",
    "**Columna obligatoria:**\n",
    "- Todos los archivos deben tener la columna `SEQN` (ID √∫nico del participante)\n",
    "\n",
    "Si no tienes los datos, el c√≥digo mostrar√° mensajes de advertencia pero no fallar√°.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚ö†Ô∏è IMPORTANTE: Los Datos NHANES NO Vienen en CSV\n",
    "\n",
    "**Los datos de NHANES se distribuyen en formato SAS Transport File (.XPT) y necesitan ser convertidos a CSV.**\n",
    "\n",
    "**Pasos para obtener los datos:**\n",
    "\n",
    "1. **Descargar archivos .XPT:**\n",
    "\n",
    "   **Opci√≥n A - Descarga Autom√°tica (Intentar primero):**\n",
    "   ```bash\n",
    "   # Descargar un m√≥dulo de prueba\n",
    "   python descargar_nhanes.py --cycle 2017-2018 --module DEMO\n",
    "   \n",
    "   # Descargar m√∫ltiples m√≥dulos\n",
    "   python descargar_nhanes.py --cycle 2017-2018 --module DEMO EXAM LAB\n",
    "   ```\n",
    "   \n",
    "   ‚ö†Ô∏è Si la descarga autom√°tica falla (com√∫n por protecciones del sitio), el script te dar√° instrucciones claras para descarga manual.\n",
    "   \n",
    "   **Opci√≥n B - Descarga Manual:**\n",
    "   - Sitio oficial: https://wwwn.cdc.gov/nchs/nhanes/Default.aspx\n",
    "   - Selecciona el ciclo (ej: 2015-2016)\n",
    "   - Descarga los m√≥dulos: DEMO, EXAM, LAB, QUEST\n",
    "   - Coloca los archivos .XPT en `./data/`\n",
    "\n",
    "2. **Convertir .XPT a CSV:**\n",
    "\n",
    "   **Opci√≥n A - Script Simple (Recomendado):**\n",
    "   ```bash\n",
    "   python convertir_nhanes.py\n",
    "   ```\n",
    "   \n",
    "   **Opci√≥n B - Script Completo:**\n",
    "   ```python\n",
    "   from nhanes_data_converter import convert_xpt_to_csv\n",
    "   from pathlib import Path\n",
    "   \n",
    "   # Convertir todos los .XPT en ./data/\n",
    "   for xpt_file in Path('./data').glob('*.XPT'):\n",
    "       convert_xpt_to_csv(xpt_file)\n",
    "   ```\n",
    "   \n",
    "   **Opci√≥n C - Manualmente:**\n",
    "   ```python\n",
    "   import pandas as pd\n",
    "   df = pd.read_sas('DEMO_2015_2016.XPT', encoding='utf-8')\n",
    "   df.to_csv('DEMO_2015_2016.csv', index=False)\n",
    "   ```\n",
    "\n",
    "3. **Colocar archivos CSV en `./data/` con el formato:**\n",
    "   - `DEMO_2007_2008.csv` (o `DEMO_J.csv` para ciclo 2017-2018)\n",
    "   - `EXAM_2007_2008.csv`\n",
    "   - `LAB_2007_2008.csv`\n",
    "   - `QUEST_2007_2008.csv`\n",
    "\n",
    "**üìñ Ver gu√≠as completas:**\n",
    "- `README.md` - Documentaci√≥n completa\n",
    "- `QUICK_START.md` - Gu√≠a de inicio r√°pido\n",
    "- `CONVERSION_DATOS_NHANES.md` - Gu√≠a detallada de conversi√≥n\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üìä FASE 1: MANEJO DE DATOS NHANES (H0.5 a H4) \n",
    "### ‚ö†Ô∏è LA PARTE M√ÅS CR√çTICA DEL HACKATHON\n",
    "\n",
    "### 1.1 Entender la estructura NHANES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### üìñ CONCEPTOS CLAVE DE NHANES:\n",
    "\n",
    "**Estructura de datos:**\n",
    "- **SEQN**: ID √∫nico de participante (clave para hacer merge)\n",
    "- **Ciclos**: Per√≠odos de 2 a√±os (2007-2008, 2009-2010, etc.)\n",
    "- **M√≥dulos separados**: Demographics, Examination, Laboratory, Questionnaire, Dietary\n",
    "\n",
    "**Pesos muestrales (WEIGHTS):**\n",
    "- `WTMEC2YR`: Peso para ex√°menes m√©dicos (2 a√±os)\n",
    "- `WTINT2YR`: Peso para entrevistas\n",
    "- `WTDRD1`: Peso para datos diet√©ticos\n",
    "- **Regla:** Usar el peso m√°s restrictivo del an√°lisis\n",
    "\n",
    "**Dise√±o complejo:**\n",
    "- `SDMVPSU`: Unidad primaria de muestreo (cluster)\n",
    "- `SDMVSTRA`: Estrato de muestreo\n",
    "- Necesario para estad√≠sticas poblacionales correctas\n",
    "\n",
    "**Variables t√≠picas:**\n",
    "- Demographics: `RIDAGEYR` (edad), `RIAGENDR` (sexo), `RIDRETH3` (etnia)\n",
    "- Examination: `BMXWT` (peso), `BMXHT` (altura), `BMXWAIST` (cintura)\n",
    "- BP: `BPXSY1`, `BPXDI1` (presi√≥n arterial)\n",
    "- Lab: `LBXGH` (A1c), `LBXGLU` (glucosa)\n",
    "- Questionnaire: `SLQ050` (sue√±o), `SMQ020` (fumador), `PAQ605` (actividad f√≠sica)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Funci√≥n de carga robusta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "CYCLE_TO_LETTER = {\n",
    "    \"2007-2008\": \"E\",\n",
    "    \"2009-2010\": \"F\",\n",
    "    \"2011-2012\": \"G\",\n",
    "    \"2013-2014\": \"H\",\n",
    "    \"2015-2016\": \"I\",\n",
    "    \"2017-2018\": \"J\",\n",
    "}\n",
    "\n",
    "def load_nhanes_data(data_dir='./data', cycles=None):\n",
    "    \"\"\"\n",
    "    Carga y merge de datos NHANES por ciclo.\n",
    "    \n",
    "    Args:\n",
    "        data_dir: Directorio con archivos CSV\n",
    "        cycles: Lista de ciclos a cargar, ej: ['2015-2016']\n",
    "    \n",
    "    Returns:\n",
    "        df: DataFrame consolidado\n",
    "        metadata: Diccionario con informaci√≥n del merge\n",
    "    \"\"\"\n",
    "    data_path = Path(data_dir)\n",
    "    \n",
    "    # Verificar que el directorio existe\n",
    "    if not data_path.exists():\n",
    "        print(f\"‚ö†Ô∏è ERROR: El directorio {data_path} no existe\")\n",
    "        print(f\"   Crea el directorio y coloca los archivos CSV all√≠\")\n",
    "        print(f\"   Estructura esperada:\")\n",
    "        print(f\"   {data_path}/\")\n",
    "        print(f\"     ‚îú‚îÄ‚îÄ DEMO_2007_2008.csv\")\n",
    "        print(f\"     ‚îú‚îÄ‚îÄ EXAM_2007_2008.csv\")\n",
    "        print(f\"     ‚îú‚îÄ‚îÄ LAB_2007_2008.csv\")\n",
    "        print(f\"     ‚îî‚îÄ‚îÄ QUEST_2007_2008.csv\")\n",
    "        return pd.DataFrame(), {'cycles': [], 'modules': [], 'n_participants': {}}\n",
    "    \n",
    "    if cycles is None:\n",
    "        # Ciclos de entrenamiento\n",
    "        cycles = ['2015-2016']\n",
    "    \n",
    "    all_data = []\n",
    "    metadata = {'cycles': cycles, 'modules': [], 'n_participants': {}}\n",
    "    \n",
    "    for cycle in cycles:\n",
    "        print(f\"\\nüìÅ Cargando ciclo {cycle}...\")\n",
    "        cycle_data = None\n",
    "        \n",
    "        # 1. DEMOGRAPHICS (siempre la base)\n",
    "        # Intentar ambos formatos: DEMO_2017_2018.csv (con gui√≥n bajo) y DEMO_J.csv (con letra)\n",
    "        letter = CYCLE_TO_LETTER.get(cycle, '')\n",
    "        demo_file = data_path / f\"DEMO_{cycle.replace('-', '_')}.csv\"\n",
    "        if not demo_file.exists() and letter:\n",
    "            # Intentar formato con letra (ej: DEMO_J.csv)\n",
    "            demo_file = data_path / f\"DEMO_{letter}.csv\"\n",
    "        if demo_file.exists():\n",
    "            try:\n",
    "                demo = pd.read_csv(demo_file)\n",
    "                if 'SEQN' not in demo.columns:\n",
    "                    print(f\"  ‚ö†Ô∏è ERROR: Columna SEQN no encontrada en {demo_file}\")\n",
    "                    print(f\"     Este archivo no es v√°lido para merge\")\n",
    "                    continue\n",
    "                demo['CYCLE'] = cycle\n",
    "                cycle_data = demo\n",
    "                print(f\"  ‚úì Demographics: {len(demo):,} registros, {len(demo.columns)} columnas\")\n",
    "            except Exception as e:\n",
    "                print(f\"  ‚ö†Ô∏è ERROR leyendo {demo_file}: {e}\")\n",
    "                continue\n",
    "        else:\n",
    "            print(f\"  ‚ö†Ô∏è Demographics no encontrado: {demo_file}\")\n",
    "            continue\n",
    "        \n",
    "        # 2. EXAMINATION (antropometr√≠a y PA)\n",
    "        # Intentar ambos formatos para EXAM\n",
    "        exam_file = data_path / f\"EXAM_{cycle.replace('-', '_')}.csv\"\n",
    "        if not exam_file.exists() and letter:\n",
    "            exam_file = data_path / f\"EXAM_{letter}.csv\"\n",
    "        if exam_file.exists():\n",
    "            try:\n",
    "                exam = pd.read_csv(exam_file)\n",
    "                if 'SEQN' in exam.columns:\n",
    "                    cycle_data = cycle_data.merge(exam, on='SEQN', how='left', suffixes=('', '_exam'))\n",
    "                    print(f\"  ‚úì Examination: {len(exam):,} registros merged, {len(exam.columns)} columnas\")\n",
    "                else:\n",
    "                    print(f\"  ‚ö†Ô∏è Columna SEQN no encontrada en {exam_file}, saltando merge\")\n",
    "            except Exception as e:\n",
    "                print(f\"  ‚ö†Ô∏è ERROR leyendo {exam_file}: {e}\")\n",
    "        \n",
    "        # 3. LABORATORY (A1c, glucosa - SOLO PARA LABEL)\n",
    "        # Intentar ambos formatos para LAB\n",
    "        lab_file = data_path / f\"LAB_{cycle.replace('-', '_')}.csv\"\n",
    "        if not lab_file.exists() and letter:\n",
    "            lab_file = data_path / f\"LAB_{letter}.csv\"\n",
    "        if lab_file.exists():\n",
    "            try:\n",
    "                lab = pd.read_csv(lab_file)\n",
    "                if 'SEQN' in lab.columns:\n",
    "                    # CR√çTICO: Marcar columnas de lab para no usarlas como features\n",
    "                    lab_cols = [c for c in lab.columns if c != 'SEQN']\n",
    "                    renamed_cols = {}\n",
    "                    for col in lab_cols:\n",
    "                        if col.startswith('LAB_'):\n",
    "                            renamed_cols[col] = col\n",
    "                        else:\n",
    "                            renamed_cols[col] = f'LAB_{col}'\n",
    "                    lab = lab.rename(columns=renamed_cols)\n",
    "                    cycle_data = cycle_data.merge(lab, on='SEQN', how='left')\n",
    "                    print(f\"  ‚úì Laboratory: {len(lab):,} registros merged (SOLO PARA LABEL), {len(lab_cols)} columnas de lab\")\n",
    "                else:\n",
    "                    print(f\"  ‚ö†Ô∏è Columna SEQN no encontrada en {lab_file}, saltando merge\")\n",
    "            except Exception as e:\n",
    "                print(f\"  ‚ö†Ô∏è ERROR leyendo {lab_file}: {e}\")\n",
    "        else:\n",
    "            print(f\"  ‚ö†Ô∏è Laboratory no encontrado: {lab_file} (opcional pero recomendado)\")\n",
    "        \n",
    "        # 4. QUESTIONNAIRE (sue√±o, actividad, tabaco)\n",
    "        # Intentar ambos formatos para QUEST\n",
    "        quest_file = data_path / f\"QUEST_{cycle.replace('-', '_')}.csv\"\n",
    "        if not quest_file.exists() and letter:\n",
    "            quest_file = data_path / f\"QUEST_{letter}.csv\"\n",
    "        if quest_file.exists():\n",
    "            try:\n",
    "                quest = pd.read_csv(quest_file)\n",
    "                if 'SEQN' in quest.columns:\n",
    "                    cycle_data = cycle_data.merge(quest, on='SEQN', how='left', suffixes=('', '_quest'))\n",
    "                    print(f\"  ‚úì Questionnaire: {len(quest):,} registros merged, {len(quest.columns)} columnas\")\n",
    "                else:\n",
    "                    print(f\"  ‚ö†Ô∏è Columna SEQN no encontrada en {quest_file}, saltando merge\")\n",
    "            except Exception as e:\n",
    "                print(f\"  ‚ö†Ô∏è ERROR leyendo {quest_file}: {e}\")\n",
    "        else:\n",
    "            print(f\"  ‚ö†Ô∏è Questionnaire no encontrado: {quest_file} (opcional)\")\n",
    "        \n",
    "        # 5. DIETARY (opcional, nutrici√≥n)\n",
    "        # Intentar ambos formatos para DIET\n",
    "        diet_file = data_path / f\"DIET_{cycle.replace('-', '_')}.csv\"\n",
    "        if not diet_file.exists() and letter:\n",
    "            diet_file = data_path / f\"DIET_{letter}.csv\"\n",
    "        if diet_file.exists():\n",
    "            try:\n",
    "                diet = pd.read_csv(diet_file)\n",
    "                if 'SEQN' in diet.columns:\n",
    "                    cycle_data = cycle_data.merge(diet, on='SEQN', how='left', suffixes=('', '_diet'))\n",
    "                    print(f\"  ‚úì Dietary: {len(diet):,} registros merged, {len(diet.columns)} columnas\")\n",
    "                else:\n",
    "                    print(f\"  ‚ö†Ô∏è Columna SEQN no encontrada en {diet_file}, saltando merge\")\n",
    "            except Exception as e:\n",
    "                print(f\"  ‚ö†Ô∏è ERROR leyendo {diet_file}: {e}\")\n",
    "        \n",
    "        if cycle_data is not None:\n",
    "            all_data.append(cycle_data)\n",
    "            metadata['n_participants'][cycle] = len(cycle_data)\n",
    "    \n",
    "    # Concatenar todos los ciclos\n",
    "    if all_data:\n",
    "        df = pd.concat(all_data, ignore_index=True)\n",
    "        print(f\"\\n‚úÖ TOTAL: {len(df):,} participantes cargados\")\n",
    "        print(f\"   Columnas totales: {df.shape[1]}\")\n",
    "        print(f\"   Ciclos cargados: {len(all_data)}/{len(cycles)}\")\n",
    "    else:\n",
    "        print(f\"\\n‚ö†Ô∏è ADVERTENCIA: No se cargaron datos de ning√∫n ciclo\")\n",
    "        print(f\"   Verifica que los archivos CSV est√©n en {data_path}\")\n",
    "        df = pd.DataFrame()\n",
    "    \n",
    "    return df, metadata\n",
    "\n",
    "TRAINING_CYCLES = ['2015-2016']\n",
    "TEST_CYCLES = ['2017-2018']\n",
    "\n",
    "# CARGAR DATOS DE ENTRENAMIENTO\n",
    "df_train, meta = load_nhanes_data(cycles=TRAINING_CYCLES)\n",
    "\n",
    "# CARGAR DATOS DE TEST (ciclo ciego)\n",
    "df_test, meta_test = load_nhanes_data(cycles=TEST_CYCLES)\n",
    "\n",
    "# Fallback para evitar bloqueos cuando a√∫n no se descarga el ciclo de entrenamiento principal\n",
    "if df_train.empty:\n",
    "    print(\"\\n‚ö†Ô∏è No se encontr√≥ el ciclo de entrenamiento 2015-2016 en ./data.\")\n",
    "    print(\"   Usando un split 80/20 del ciclo 2017-2018 solo para exploraci√≥n.\")\n",
    "    if df_test.empty:\n",
    "        raise RuntimeError(\"No hay datos disponibles en ./data. Ejecuta los scripts de descarga antes de continuar.\")\n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "    df_train, df_test = train_test_split(\n",
    "        df_test,\n",
    "        test_size=0.2,\n",
    "        random_state=42,\n",
    "        shuffle=True,\n",
    "    )\n",
    "    df_train = df_train.reset_index(drop=True)\n",
    "    df_test = df_test.reset_index(drop=True)\n",
    "    meta = {\n",
    "        'cycles': ['2017-2018 (split_train)'],\n",
    "        'n_participants': {'2017-2018_split_train': len(df_train)},\n",
    "        'fallback': True,\n",
    "    }\n",
    "    meta_test = {\n",
    "        'cycles': ['2017-2018 (split_test)'],\n",
    "        'n_participants': {'2017-2018_split_test': len(df_test)},\n",
    "        'fallback': True,\n",
    "    }\n",
    "else:\n",
    "    if df_test.empty:\n",
    "        raise RuntimeError(\"El ciclo de test 2017-2018 no est√° disponible. Desc√°rgalo antes de continuar.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Exploraci√≥n inicial cr√≠tica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Verificar distribuci√≥n por ciclo\n",
    "print(\"üìä Distribuci√≥n por ciclo:\")\n",
    "print(df_train['CYCLE'].value_counts().sort_index())\n",
    "print(f\"\\nüìä Test set: {df_test['CYCLE'].value_counts()}\")\n",
    "\n",
    "# Variables clave disponibles\n",
    "print(\"\\nüîë Variables clave disponibles:\")\n",
    "key_vars = {\n",
    "    'Demographics': ['RIDAGEYR', 'RIAGENDR', 'RIDRETH3'],\n",
    "    'Anthropometry': ['BMXWT', 'BMXHT', 'BMXWAIST', 'BMXBMI'],\n",
    "    'Blood Pressure': ['BPXSY1', 'BPXSY2', 'BPXDI1', 'BPXDI2'],\n",
    "    'Laboratory': ['LAB_LBXGH', 'LAB_LBXGLU'],  # Prefijo LAB_\n",
    "    'Sleep': ['SLD010H', 'SLD012'],\n",
    "    'Smoking': ['SMQ020', 'SMD030'],\n",
    "    'Physical Activity': ['PAQ605', 'PAQ620', 'PAD680']\n",
    "}\n",
    "\n",
    "for module, vars in key_vars.items():\n",
    "    available = [v for v in vars if v in df_train.columns]\n",
    "    print(f\"  {module}: {len(available)}/{len(vars)} disponibles\")\n",
    "    if len(available) < len(vars):\n",
    "        missing = [v for v in vars if v not in df_train.columns]\n",
    "        print(f\"    ‚ö†Ô∏è Faltantes: {missing}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Limpieza y tipado de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "def clean_nhanes_data(df):\n",
    "    \"\"\"\n",
    "    Limpieza est√°ndar de datos NHANES.\n",
    "    \n",
    "    Maneja:\n",
    "    - Valores missing codificados (77777, 99999, etc.)\n",
    "    - Conversi√≥n de tipos\n",
    "    - Rangos v√°lidos\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # 1. Valores missing codificados en NHANES\n",
    "    missing_codes = [7, 9, 77, 99, 777, 999, 7777, 9999, 77777, 99999, '.', '']\n",
    "    for col in df.select_dtypes(include=[np.number]).columns:\n",
    "        df[col] = df[col].replace(missing_codes, np.nan)\n",
    "    \n",
    "    # 2. Edad: solo adultos\n",
    "    if 'RIDAGEYR' in df.columns:\n",
    "        df = df[df['RIDAGEYR'] >= 18].copy()\n",
    "        df = df[df['RIDAGEYR'] <= 85].copy()  # L√≠mite superior\n",
    "    \n",
    "    # 3. Sexo: 1=M, 2=F\n",
    "    if 'RIAGENDR' in df.columns:\n",
    "        df['sex'] = df['RIAGENDR'].map({1: 'M', 2: 'F'})\n",
    "    \n",
    "    # 4. Antropometr√≠a: rangos razonables\n",
    "    if 'BMXWT' in df.columns:  # Peso en kg\n",
    "        df.loc[(df['BMXWT'] < 30) | (df['BMXWT'] > 250), 'BMXWT'] = np.nan\n",
    "    \n",
    "    if 'BMXHT' in df.columns:  # Altura en cm\n",
    "        df.loc[(df['BMXHT'] < 120) | (df['BMXHT'] > 220), 'BMXHT'] = np.nan\n",
    "    \n",
    "    if 'BMXWAIST' in df.columns:  # Cintura en cm\n",
    "        df.loc[(df['BMXWAIST'] < 40) | (df['BMXWAIST'] > 200), 'BMXWAIST'] = np.nan\n",
    "    \n",
    "    # 5. Presi√≥n arterial: rangos v√°lidos\n",
    "    for bp_col in ['BPXSY1', 'BPXSY2', 'BPXSY3']:\n",
    "        if bp_col in df.columns:\n",
    "            df.loc[(df[bp_col] < 70) | (df[bp_col] > 250), bp_col] = np.nan\n",
    "    \n",
    "    for bp_col in ['BPXDI1', 'BPXDI2', 'BPXDI3']:\n",
    "        if bp_col in df.columns:\n",
    "            df.loc[(df[bp_col] < 30) | (df[bp_col] > 150), bp_col] = np.nan\n",
    "    \n",
    "    print(f\"‚úÖ Limpieza completada: {len(df):,} registros v√°lidos\")\n",
    "    return df\n",
    "\n",
    "# Aplicar limpieza\n",
    "df_train = clean_nhanes_data(df_train)\n",
    "df_test = clean_nhanes_data(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 ‚ö†Ô∏è ANTI-FUGA: Identificar columnas de laboratorio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# CR√çTICO: Identificar todas las columnas de laboratorio\n",
    "LAB_COLUMNS = [col for col in df_train.columns if col.startswith('LAB_')]\n",
    "\n",
    "print(f\"üö® COLUMNAS DE LABORATORIO IDENTIFICADAS (NO USAR COMO FEATURES):\")\n",
    "print(f\"   Total: {len(LAB_COLUMNS)} columnas\")\n",
    "for col in LAB_COLUMNS[:20]:  # Mostrar primeras 20\n",
    "    print(f\"   - {col}\")\n",
    "\n",
    "# Guardar lista para referencia\n",
    "with open('LAB_COLUMNS_FORBIDDEN.txt', 'w') as f:\n",
    "    f.write(\"\\n\".join(LAB_COLUMNS))\n",
    "\n",
    "print(\"\\n‚úÖ Lista guardada en LAB_COLUMNS_FORBIDDEN.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üéØ FASE 2: CREACI√ìN DE LABELS (H4 a H5)\n",
    "\n",
    "### 2.1 Label A: Alto riesgo de diabetes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# üîí Control global de columnas prohibidas derivadas del label\n",
    "FORBIDDEN_LABEL_FEATURES: set[str] = set()\n",
    "FORBIDDEN_LABEL_TOKENS: set[str] = {\n",
    "    \"lbxgh\",\n",
    "    \"lbxglu\",\n",
    "    \"hba1c\",\n",
    "    \"a1c\",\n",
    "    \"glucose\",\n",
    "    \"glu\",\n",
    "}\n",
    "\n",
    "def register_forbidden_label_features(columns: list[str]) -> None:\n",
    "    \"\"\"Registra columnas y tokens que no pueden usarse como features.\"\"\"\n",
    "    global FORBIDDEN_LABEL_FEATURES, FORBIDDEN_LABEL_TOKENS\n",
    "    for col in columns:\n",
    "        if not col:\n",
    "            continue\n",
    "        col_str = str(col)\n",
    "        FORBIDDEN_LABEL_FEATURES.add(col_str)\n",
    "        col_lower = col_str.lower()\n",
    "        FORBIDDEN_LABEL_TOKENS.update({\n",
    "            col_lower,\n",
    "            col_lower.replace(\"lab_\", \"\"),\n",
    "            col_lower.replace(\"_\", \"\"),\n",
    "            col_lower.split(\"lab_\")[-1],\n",
    "        })\n",
    "\n",
    "def feature_has_forbidden_token(feature_name: str) -> bool:\n",
    "    \"\"\"Detecta si un nombre de feature contiene tokens prohibidos del label.\"\"\"\n",
    "    name = str(feature_name).lower()\n",
    "    return any(token and token in name for token in FORBIDDEN_LABEL_TOKENS)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# ‚úÖ Features permitidas expl√≠citamente (demograf√≠a + estilo de vida)\n",
    "ALLOWED_FEATURES: list[str] = [\n",
    "    # Demogr√°ficas\n",
    "    'age', 'age_squared', 'sex_male',\n",
    "    # Antropometr√≠a\n",
    "    'bmi', 'bmi_squared', 'waist_height_ratio', 'waist_height_ratio_squared',\n",
    "    'high_waist_height_ratio', 'central_obesity', 'high_risk_profile',\n",
    "    # Estilo de vida\n",
    "    'sleep_hours', 'poor_sleep',\n",
    "    'cigarettes_per_day', 'current_smoker', 'ever_smoker',\n",
    "    'total_active_days', 'meets_activity_guidelines', 'sedentary_flag',\n",
    "    'lifestyle_risk_score',\n",
    "    # Interacciones y derivados v√°lidos\n",
    "    'bmi_age_interaction', 'waist_age_interaction',\n",
    "    'bmi_age_sex_interaction', 'obesity_sedentary_combo',\n",
    "    'age_poor_sleep', 'triple_risk',\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "def create_diabetes_label(df):\n",
    "    \"\"\"\n",
    "    Label A: Alto riesgo de diabetes usando HbA1c y/o glucosa en ayunas.\n",
    "\n",
    "    Basado en ADA Standards of Care 2024 y restricciones del desaf√≠o:\n",
    "      - HbA1c >= 6.0% ‚Üí Riesgo alto (incluye prediabetes avanzada y diabetes)\n",
    "      - Glucosa en ayunas >= 110 mg/dL ‚Üí Riesgo alto (IFG moderado/alto + diabetes)\n",
    "\n",
    "    Returns:\n",
    "        tuple[pd.DataFrame, list[str]]: dataframe filtrado y columnas prohibidas desde el label\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    def _resolve_column(df, candidates):\n",
    "        for name in candidates:\n",
    "            if name in df.columns:\n",
    "                return name\n",
    "        return None\n",
    "\n",
    "    a1c_col = _resolve_column(df, ['LAB_LBXGH', 'LAB_LAB_LBXGH', 'LBXGH', 'HBA1C', 'A1C'])\n",
    "    glucose_col = _resolve_column(df, ['LAB_LBXGLU', 'LAB_LAB_LBXGLU', 'LBXGLU', 'GLUCOSE', 'GLU_FAST'])\n",
    "\n",
    "    df['label_diabetes'] = 0\n",
    "\n",
    "    has_a1c = a1c_col is not None\n",
    "    has_glucose = glucose_col is not None\n",
    "\n",
    "    if not has_a1c and not has_glucose:\n",
    "        print(\"‚ö†Ô∏è ADVERTENCIA: No se encontraron columnas LAB_LBXGH o LAB_LBXGLU\")\n",
    "        print(\"   Verifica que los archivos LAB_*.csv se hayan combinado correctamente\")\n",
    "        return df, []\n",
    "\n",
    "    forbidden_from_label = []\n",
    "\n",
    "    if has_a1c:\n",
    "        valid_a1c = df[a1c_col].notna()\n",
    "        n_a1c = (valid_a1c & (df[a1c_col] >= 6.0)).sum()\n",
    "        df.loc[valid_a1c & (df[a1c_col] >= 6.0), 'label_diabetes'] = 1\n",
    "        forbidden_from_label.extend([a1c_col, 'LBXGH', 'HBA1C', 'A1C'])\n",
    "        print(f\"  HbA1c >= 6.0%: {n_a1c:,} casos (columna: {a1c_col})\")\n",
    "    else:\n",
    "        print(\"  ‚ö†Ô∏è Ninguna columna de HbA1c encontrada en el dataset\")\n",
    "\n",
    "    if has_glucose:\n",
    "        valid_glu = df[glucose_col].notna()\n",
    "        n_glu = (valid_glu & (df[glucose_col] >= 110)).sum()\n",
    "        df.loc[valid_glu & (df[glucose_col] >= 110), 'label_diabetes'] = 1\n",
    "        forbidden_from_label.extend([glucose_col, 'LBXGLU', 'GLUCOSE', 'GLU_FAST'])\n",
    "        print(f\"  Glucosa >= 110 mg/dL: {n_glu:,} casos (columna: {glucose_col})\")\n",
    "    else:\n",
    "        print(\"  ‚ö†Ô∏è Ninguna columna de glucosa en ayunas encontrada en el dataset\")\n",
    "\n",
    "    lab_cols = []\n",
    "    if has_a1c:\n",
    "        lab_cols.append(a1c_col)\n",
    "    if has_glucose:\n",
    "        lab_cols.append(glucose_col)\n",
    "\n",
    "    if lab_cols:\n",
    "        has_lab_data = df[lab_cols].notna().any(axis=1)\n",
    "        df = df[has_lab_data].copy()\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è ADVERTENCIA: No hay datos de laboratorio disponibles para crear la etiqueta\")\n",
    "        return df, forbidden_from_label\n",
    "\n",
    "    prevalence = df['label_diabetes'].mean()\n",
    "    print(\"\\n‚úÖ Label Diabetes creado\")\n",
    "    print(f\"   Prevalencia: {prevalence:.1%}\")\n",
    "    print(f\"   Casos positivos: {df['label_diabetes'].sum():,} / {len(df):,}\")\n",
    "\n",
    "    register_forbidden_label_features(forbidden_from_label)\n",
    "\n",
    "    return df, forbidden_from_label\n",
    "\n",
    "# Crear labels\n",
    "df_train, forbidden_label_features_train = create_diabetes_label(df_train)\n",
    "df_test, forbidden_label_features_test = create_diabetes_label(df_test)\n",
    "\n",
    "FORBIDDEN_LABEL_FEATURES.update(forbidden_label_features_train)\n",
    "FORBIDDEN_LABEL_FEATURES.update(forbidden_label_features_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Label B (alternativa): Hipertensi√≥n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Indicador binario de diabetes utilizable en la UI\n",
    "for split_name, df_split in [('Train', df_train), ('Test', df_test)]:\n",
    "    df_split.loc[:, 'has_diabetes'] = df_split['label_diabetes'].map({1: 'S√≠', 0: 'No'})\n",
    "    counts = df_split['has_diabetes'].value_counts().rename('personas')\n",
    "    percents = (df_split['has_diabetes'].value_counts(normalize=True) * 100).round(1).rename('%')\n",
    "    summary = pd.concat([counts, percents], axis=1)\n",
    "    print(f\"üë§ {split_name}: distribuci√≥n de has_diabetes\")\n",
    "    print(summary)\n",
    "    print()\n",
    "\n",
    "print(\"üìå Ejemplo de salida (test):\")\n",
    "df_test[['SEQN', 'has_diabetes']].head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "def create_hypertension_label(df):\n",
    "    \"\"\"\n",
    "    Etiqueta de hipertensi√≥n usando mediciones de PA.\n",
    "    \n",
    "    Criterios (Estadio 1+):\n",
    "    - Sist√≥lica >= 130 mmHg, o\n",
    "    - Diast√≥lica >= 80 mmHg\n",
    "    \n",
    "    Usa promedio de 2-3 mediciones disponibles.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Calcular promedio de mediciones de PA\n",
    "    sys_cols = [c for c in df.columns if c.startswith('BPXSY') and len(c) > 5 and c[5:].isdigit()]\n",
    "    dia_cols = [c for c in df.columns if c.startswith('BPXDI') and len(c) > 5 and c[5:].isdigit()]\n",
    "    \n",
    "    if not sys_cols and not dia_cols:\n",
    "        print(\"‚ö†Ô∏è ADVERTENCIA: No se encontraron columnas de presi√≥n arterial (BPXSY* o BPXDI*)\")\n",
    "        print(\"   Aseg√∫rate de que los archivos EXAM_*.csv est√©n cargados correctamente\")\n",
    "        return df\n",
    "    \n",
    "    # Inicializar columnas\n",
    "    if sys_cols:\n",
    "        df['sbp_mean'] = df[sys_cols].mean(axis=1)\n",
    "        print(f\"  Columnas sist√≥lica encontradas: {sys_cols}\")\n",
    "    else:\n",
    "        df['sbp_mean'] = np.nan\n",
    "        print(\"  ‚ö†Ô∏è No se encontraron columnas de presi√≥n sist√≥lica\")\n",
    "    \n",
    "    if dia_cols:\n",
    "        df['dbp_mean'] = df[dia_cols].mean(axis=1)\n",
    "        print(f\"  Columnas diast√≥lica encontradas: {dia_cols}\")\n",
    "    else:\n",
    "        df['dbp_mean'] = np.nan\n",
    "        print(\"  ‚ö†Ô∏è No se encontraron columnas de presi√≥n diast√≥lica\")\n",
    "    \n",
    "    # Criterio de hipertensi√≥n (solo si tenemos al menos una medida)\n",
    "    df['label_hypertension'] = 0\n",
    "    has_bp_data = df[['sbp_mean', 'dbp_mean']].notna().any(axis=1)\n",
    "    \n",
    "    if has_bp_data.any():\n",
    "        # Aplicar criterio solo donde hay datos\n",
    "        mask = (df['sbp_mean'].notna() & (df['sbp_mean'] >= 130)) | \\\n",
    "               (df['dbp_mean'].notna() & (df['dbp_mean'] >= 80))\n",
    "        df.loc[mask, 'label_hypertension'] = 1\n",
    "    \n",
    "    # Remover casos sin datos de PA\n",
    "    df = df[has_bp_data].copy()\n",
    "    \n",
    "    if len(df) > 0:\n",
    "        prevalence = df['label_hypertension'].mean()\n",
    "        print(f\"\\n‚úÖ Label Hipertensi√≥n creado:\")\n",
    "        print(f\"   Prevalencia: {prevalence:.1%}\")\n",
    "        print(f\"   n = {df['label_hypertension'].sum():,} / {len(df):,}\")\n",
    "    else:\n",
    "        print(\"\\n‚ö†Ô∏è ADVERTENCIA: No hay datos de presi√≥n arterial disponibles\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Opcional: crear tambi√©n label de hipertensi√≥n\n",
    "df_train = create_hypertension_label(df_train)\n",
    "df_test = create_hypertension_label(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üîß FASE 3: INGENIER√çA DE FEATURES (H5 a H7)\n",
    "\n",
    "### 3.1 Features demogr√°ficas y antropom√©tricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# üîÑ Redefinir engineer_features para usar solo ALLOWED_FEATURES\n",
    "\n",
    "def engineer_features(df):\n",
    "    \"\"\"Crea features derivadas v√°lidas basadas en demograf√≠a y estilo de vida.\"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    # 1. ANTROPOMETR√çA\n",
    "    if {'BMXWT', 'BMXHT'} <= set(df.columns):\n",
    "        df['bmi'] = df['BMXWT'] / ((df['BMXHT'] / 100) ** 2)\n",
    "        df['bmi_category'] = pd.cut(\n",
    "            df['bmi'],\n",
    "            bins=[0, 18.5, 25, 30, 100],\n",
    "            labels=['underweight', 'normal', 'overweight', 'obese']\n",
    "        )\n",
    "    if {'BMXWAIST', 'BMXHT'} <= set(df.columns):\n",
    "        df['waist_height_ratio'] = df['BMXWAIST'] / df['BMXHT']\n",
    "        df['high_waist_height_ratio'] = (df['waist_height_ratio'] >= 0.5).astype(int)\n",
    "\n",
    "    # 2. EDAD\n",
    "    if 'RIDAGEYR' in df.columns:\n",
    "        df['age'] = df['RIDAGEYR']\n",
    "        df['age_group'] = pd.cut(df['age'], bins=[0, 30, 45, 60, 100], labels=['18-30', '31-45', '46-60', '60+'])\n",
    "        df['age_squared'] = df['age'] ** 2\n",
    "\n",
    "    # 3. SEXO\n",
    "    if 'sex' in df.columns:\n",
    "        df['sex_male'] = (df['sex'] == 'M').astype(int)\n",
    "\n",
    "    # 4. SUE√ëO\n",
    "    for col in ['SLD010H', 'SLD012']:\n",
    "        if col in df.columns:\n",
    "            df['sleep_hours'] = df[col]\n",
    "            df['poor_sleep'] = ((df['sleep_hours'] < 7) | (df['sleep_hours'] > 9)).astype(int)\n",
    "            break\n",
    "\n",
    "    # 5. TABAQUISMO\n",
    "    if 'SMQ020' in df.columns:\n",
    "        df['ever_smoker'] = (df['SMQ020'] == 1).astype(int)\n",
    "    if 'SMD030' in df.columns:\n",
    "        df['cigarettes_per_day'] = df['SMD030']\n",
    "        df['current_smoker'] = (df['cigarettes_per_day'] > 0).astype(int)\n",
    "\n",
    "    # 6. ACTIVIDAD F√çSICA\n",
    "    if {'PAQ605', 'PAQ620'} <= set(df.columns):\n",
    "        df['total_active_days'] = df['PAQ605'].fillna(0) + df['PAQ620'].fillna(0)\n",
    "        df['meets_activity_guidelines'] = (df['total_active_days'] >= 5).astype(int)\n",
    "        df['sedentary_flag'] = (df['meets_activity_guidelines'] == 0).astype(int)\n",
    "\n",
    "    # 7. INTERACCIONES\n",
    "    if {'bmi', 'age'} <= set(df.columns):\n",
    "        df['bmi_age_interaction'] = df['bmi'] * df['age']\n",
    "        df['high_risk_profile'] = ((df['bmi'] >= 30) & (df['age'] >= 45)).astype(int)\n",
    "    if {'waist_height_ratio', 'age'} <= set(df.columns):\n",
    "        df['waist_age_interaction'] = df['waist_height_ratio'] * df['age']\n",
    "\n",
    "    # 8. OBESIDAD CENTRAL\n",
    "    if {'BMXWAIST', 'sex'} <= set(df.columns):\n",
    "        df['central_obesity'] = np.where(\n",
    "            df['BMXWAIST'].notna(),\n",
    "            np.where(\n",
    "                ((df['sex'] == 'M') & (df['BMXWAIST'] >= 102)) |\n",
    "                ((df['sex'] == 'F') & (df['BMXWAIST'] >= 88)),\n",
    "                1,\n",
    "                0\n",
    "            ),\n",
    "            np.nan\n",
    "        )\n",
    "\n",
    "    # 9. PUNTAJE DE ESTILO DE VIDA\n",
    "    lifestyle_components = []\n",
    "    for col in ['current_smoker', 'poor_sleep', 'sedentary_flag']:\n",
    "        if col in df.columns:\n",
    "            lifestyle_components.append(df[col].fillna(0))\n",
    "    if lifestyle_components:\n",
    "        df['lifestyle_risk_score'] = np.vstack(lifestyle_components).sum(axis=0)\n",
    "\n",
    "    # 10. FEATURES AVANZADAS\n",
    "    if {'bmi', 'age', 'sex_male'} <= set(df.columns):\n",
    "        df['bmi_age_sex_interaction'] = df['bmi'] * df['age'] * df['sex_male']\n",
    "    if 'waist_height_ratio' in df.columns:\n",
    "        df['waist_height_ratio_squared'] = df['waist_height_ratio'] ** 2\n",
    "    if 'bmi' in df.columns:\n",
    "        df['bmi_squared'] = df['bmi'] ** 2\n",
    "    if {'central_obesity', 'sedentary_flag'} <= set(df.columns):\n",
    "        df['obesity_sedentary_combo'] = df['central_obesity'] * df['sedentary_flag']\n",
    "    if {'age', 'poor_sleep'} <= set(df.columns):\n",
    "        df['age_poor_sleep'] = df['age'] * df['poor_sleep']\n",
    "    if {'central_obesity', 'age', 'current_smoker'} <= set(df.columns):\n",
    "        df['triple_risk'] = df['central_obesity'] * (df['age'] >= 45).astype(int) * df['current_smoker']\n",
    "\n",
    "    # Garantizar que todas las features permitidas existan\n",
    "    for feature in ALLOWED_FEATURES:\n",
    "        if feature not in df.columns:\n",
    "            df[feature] = np.nan\n",
    "\n",
    "    print(f\"‚úÖ Features creadas: {df.shape[1]} columnas totales\")\n",
    "\n",
    "    available_features = [f for f in ALLOWED_FEATURES if f in df.columns]\n",
    "    print(f\"\\nüìä Features num√©ricas disponibles: {len(available_features)}\")\n",
    "    for feat in available_features:\n",
    "        missing_pct = df[feat].isna().mean() * 100\n",
    "        print(f\"   - {feat}: {missing_pct:.1f}% missing\")\n",
    "\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "def engineer_features(df):\n",
    "    \"\"\"\n",
    "    Crea features derivadas SOLO de variables permitidas.\n",
    "    \n",
    "    NO usar columnas de laboratorio (LAB_*).\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # 1. ANTROPOMETR√çA\n",
    "    # IMC (Body Mass Index)\n",
    "    if 'BMXWT' in df.columns and 'BMXHT' in df.columns:\n",
    "        df['bmi'] = df['BMXWT'] / ((df['BMXHT'] / 100) ** 2)\n",
    "        # Categor√≠as de IMC\n",
    "        df['bmi_category'] = pd.cut(df['bmi'], \n",
    "                                     bins=[0, 18.5, 25, 30, 100],\n",
    "                                     labels=['underweight', 'normal', 'overweight', 'obese'])\n",
    "    \n",
    "    # Ratio cintura-altura (mejor predictor que IMC)\n",
    "    if 'BMXWAIST' in df.columns and 'BMXHT' in df.columns:\n",
    "        df['waist_height_ratio'] = df['BMXWAIST'] / df['BMXHT']\n",
    "        # Riesgo si >= 0.5\n",
    "        df['high_waist_height_ratio'] = (df['waist_height_ratio'] >= 0.5).astype(int)\n",
    "    \n",
    "    # 2. EDAD\n",
    "    if 'RIDAGEYR' in df.columns:\n",
    "        df['age'] = df['RIDAGEYR']\n",
    "        # Grupos etarios\n",
    "        df['age_group'] = pd.cut(df['age'], \n",
    "                                  bins=[0, 30, 45, 60, 100],\n",
    "                                  labels=['18-30', '31-45', '46-60', '60+'])\n",
    "        # Edad al cuadrado (relaci√≥n no lineal)\n",
    "        df['age_squared'] = df['age'] ** 2\n",
    "    \n",
    "    # 3. SEXO (ya mapeado en limpieza)\n",
    "    if 'sex' in df.columns:\n",
    "        df['sex_male'] = (df['sex'] == 'M').astype(int)\n",
    "    \n",
    "    # 4. SUE√ëO\n",
    "    sleep_cols = ['SLD010H', 'SLD012']  # Horas de sue√±o\n",
    "    for col in sleep_cols:\n",
    "        if col in df.columns:\n",
    "            df['sleep_hours'] = df[col]\n",
    "            # Sue√±o insuficiente (<7h) o excesivo (>9h)\n",
    "            df['poor_sleep'] = ((df['sleep_hours'] < 7) | (df['sleep_hours'] > 9)).astype(int)\n",
    "            break\n",
    "    \n",
    "    # 5. TABAQUISMO\n",
    "    if 'SMQ020' in df.columns:  # ¬øHa fumado 100+ cigarrillos?\n",
    "        df['ever_smoker'] = (df['SMQ020'] == 1).astype(int)\n",
    "\n",
    "    if 'SMD030' in df.columns:  # Cigarrillos por d√≠a\n",
    "        df['cigarettes_per_day'] = df['SMD030']\n",
    "        df['current_smoker'] = (df['cigarettes_per_day'] > 0).astype(int)\n",
    "\n",
    "    # 6. ACTIVIDAD F√çSICA\n",
    "    if 'PAQ605' in df.columns and 'PAQ620' in df.columns:\n",
    "        df['total_active_days'] = df['PAQ605'].fillna(0) + df['PAQ620'].fillna(0)\n",
    "        # Cumple recomendaciones (150+ min/semana ‚âà 5 d√≠as)\n",
    "        df['meets_activity_guidelines'] = (df['total_active_days'] >= 5).astype(int)\n",
    "        df['sedentary_flag'] = (df['meets_activity_guidelines'] == 0).astype(int)\n",
    "\n",
    "    # 7. INTERACCIONES IMPORTANTES\n",
    "    if 'bmi' in df.columns and 'age' in df.columns:\n",
    "        df['bmi_age_interaction'] = df['bmi'] * df['age']\n",
    "        df['high_risk_profile'] = ((df['bmi'] >= 30) & (df['age'] >= 45)).astype(int)\n",
    "\n",
    "    if 'waist_height_ratio' in df.columns and 'age' in df.columns:\n",
    "        df['waist_age_interaction'] = df['waist_height_ratio'] * df['age']\n",
    "\n",
    "    # 8. OBESIDAD CENTRAL SEG√öN SEXO\n",
    "    if 'BMXWAIST' in df.columns and 'sex' in df.columns:\n",
    "        df['central_obesity'] = np.where(\n",
    "            df['BMXWAIST'].notna(),\n",
    "            np.where(\n",
    "                ((df['sex'] == 'M') & (df['BMXWAIST'] >= 102)) |\n",
    "                ((df['sex'] == 'F') & (df['BMXWAIST'] >= 88)),\n",
    "                1,\n",
    "                0\n",
    "            ),\n",
    "            np.nan\n",
    "        )\n",
    "\n",
    "    # 9. PUNTAJE DE ESTILO DE VIDA\n",
    "    lifestyle_components = []\n",
    "    if 'current_smoker' in df.columns:\n",
    "        lifestyle_components.append(df['current_smoker'].fillna(0))\n",
    "    if 'poor_sleep' in df.columns:\n",
    "        lifestyle_components.append(df['poor_sleep'].fillna(0))\n",
    "    if 'sedentary_flag' in df.columns:\n",
    "        lifestyle_components.append(df['sedentary_flag'].fillna(0))\n",
    "\n",
    "    if lifestyle_components:\n",
    "        df['lifestyle_risk_score'] = np.vstack(lifestyle_components).sum(axis=0)\n",
    "\n",
    "    # 10. FEATURES ADICIONALES AVANZADAS (Para mejorar AUROC)\n",
    "    # Interacciones de 3er orden\n",
    "    if 'bmi' in df.columns and 'age' in df.columns and 'sex_male' in df.columns:\n",
    "        df['bmi_age_sex_interaction'] = df['bmi'] * df['age'] * df['sex_male']\n",
    "    \n",
    "    # Cintura cuadr√°tica (relaci√≥n no lineal)\n",
    "    if 'waist_height_ratio' in df.columns:\n",
    "        df['waist_height_ratio_squared'] = df['waist_height_ratio'] ** 2\n",
    "    \n",
    "    # BMI cuadr√°tico\n",
    "    if 'bmi' in df.columns:\n",
    "        df['bmi_squared'] = df['bmi'] ** 2\n",
    "    \n",
    "    # Perfil de riesgo compuesto\n",
    "    if 'central_obesity' in df.columns and 'sedentary_flag' in df.columns:\n",
    "        df['obesity_sedentary_combo'] = df['central_obesity'] * df['sedentary_flag']\n",
    "    \n",
    "    # Edad y sue√±o\n",
    "    if 'age' in df.columns and 'poor_sleep' in df.columns:\n",
    "        df['age_poor_sleep'] = df['age'] * df['poor_sleep']\n",
    "    \n",
    "    # Obesidad, edad y tabaco (triple riesgo)\n",
    "    if 'central_obesity' in df.columns and 'age' in df.columns and 'current_smoker' in df.columns:\n",
    "        df['triple_risk'] = df['central_obesity'] * (df['age'] >= 45).astype(int) * df['current_smoker']\n",
    "\n",
    "    print(f\"‚úÖ Features creadas: {df.shape[1]} columnas totales\")\n",
    "\n",
    "    # Listar nuevas features num√©ricas\n",
    "    new_features = ['bmi', 'waist_height_ratio', 'central_obesity', 'age', 'age_squared',\n",
    "                    'sleep_hours', 'cigarettes_per_day', 'total_active_days', 'lifestyle_risk_score',\n",
    "                    'bmi_age_interaction', 'waist_age_interaction', 'high_risk_profile', 'sedentary_flag',\n",
    "                    'bmi_age_sex_interaction', 'waist_height_ratio_squared', 'bmi_squared',\n",
    "                    'obesity_sedentary_combo', 'age_poor_sleep', 'triple_risk']\n",
    "    available_features = [f for f in new_features if f in df.columns]\n",
    "\n",
    "    print(f\"\\nüìä Features num√©ricas disponibles: {len(available_features)}\")\n",
    "    for feat in available_features:\n",
    "        if feat in df.columns:\n",
    "            missing_pct = df[feat].isna().mean() * 100\n",
    "            print(f\"   - {feat}: {missing_pct:.1f}% missing\")\n",
    "\n",
    "    return df\n",
    "\n",
    "# Aplicar ingenier√≠a de features\n",
    "df_train = engineer_features(df_train)\n",
    "df_test = engineer_features(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Selecci√≥n final de features (sin fuga)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# üîí Redefinir select_features_no_leakage usando ALLOWED_FEATURES\n",
    "\n",
    "def select_features_no_leakage(df, target='label_diabetes'):\n",
    "    \"\"\"Selecciona features v√°lidas y aplica chequeos estrictos anti-fuga.\"\"\"\n",
    "    candidate_features = [f for f in ALLOWED_FEATURES if f in df.columns]\n",
    "\n",
    "    lab_features = [f for f in candidate_features if f.startswith('LAB_')]\n",
    "    if lab_features:\n",
    "        raise ValueError(f\"üö® FUGA DE DATOS DETECTADA: {lab_features}\")\n",
    "\n",
    "    forbidden_direct = [f for f in candidate_features if f in FORBIDDEN_LABEL_FEATURES]\n",
    "    forbidden_tokens = [f for f in candidate_features if feature_has_forbidden_token(f)]\n",
    "    if forbidden_direct or forbidden_tokens:\n",
    "        raise ValueError(\n",
    "            \"üö® FUGA DE DATOS DETECTADA por label: \"\n",
    "            f\"Directas={forbidden_direct}, Derivadas={forbidden_tokens}\"\n",
    "        )\n",
    "\n",
    "    print(f\"‚úÖ Features seleccionadas: {len(candidate_features)}\")\n",
    "    print(f\"   Target: {target}\")\n",
    "\n",
    "    X = df[candidate_features].copy()\n",
    "    y = df[target].copy()\n",
    "\n",
    "    valid_idx = y.notna()\n",
    "    X = X[valid_idx]\n",
    "    y = y[valid_idx]\n",
    "\n",
    "    print(f\"\\nüìä Dataset final: {len(X):,} registros\")\n",
    "    print(f\"   Prevalencia: {y.mean():.1%}\")\n",
    "    print(f\"   Clase positiva: {int(y.sum()):,} | Clase negativa: {int(len(y) - y.sum()):,}\")\n",
    "\n",
    "    return X, y, candidate_features\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "def legacy_select_features_no_leakage(df, target='label_diabetes'):\n",
    "    \"\"\"\n",
    "    Selecciona features finales SIN columnas de laboratorio ni derivadas del label.\n",
    "    \n",
    "    CR√çTICO: Valida que no haya fuga de datos.\n",
    "    \"\"\"\n",
    "    # Features candidatas (ajustar seg√∫n disponibilidad)\n",
    "    candidate_features = [\n",
    "        # Demogr√°ficas\n",
    "        'age', 'age_squared', 'sex_male',\n",
    "\n",
    "        # Antropometr√≠a\n",
    "        'bmi', 'bmi_squared', 'waist_height_ratio', 'waist_height_ratio_squared',\n",
    "        'high_waist_height_ratio', 'central_obesity', 'high_risk_profile',\n",
    "\n",
    "        # Estilo de vida\n",
    "        'sleep_hours', 'poor_sleep',\n",
    "        'cigarettes_per_day', 'current_smoker', 'ever_smoker',\n",
    "        'total_active_days', 'meets_activity_guidelines', 'sedentary_flag',\n",
    "        'lifestyle_risk_score',\n",
    "\n",
    "        # Interacciones de 2do orden\n",
    "        'bmi_age_interaction', 'waist_age_interaction',\n",
    "        \n",
    "        # Interacciones avanzadas (3er orden y cuadr√°ticas)\n",
    "        'bmi_age_sex_interaction', 'obesity_sedentary_combo',\n",
    "        'age_poor_sleep', 'triple_risk',\n",
    "\n",
    "        # Presi√≥n arterial (si el label NO es hipertensi√≥n)\n",
    "        # 'sbp_mean', 'dbp_mean'  # Descomentar si label = diabetes\n",
    "    ]\n",
    "    \n",
    "    # Filtrar features disponibles\n",
    "    features = [f for f in candidate_features if f in df.columns]\n",
    "    \n",
    "    # VALIDACI√ìN ANTI-FUGA\n",
    "    lab_features = [f for f in features if f.startswith('LAB_')]\n",
    "    if lab_features:\n",
    "        raise ValueError(f\"üö® FUGA DE DATOS DETECTADA: {lab_features}\")\n",
    "    \n",
    "    forbidden_direct = [f for f in features if f in FORBIDDEN_LABEL_FEATURES]\n",
    "    forbidden_tokens = [f for f in features if feature_has_forbidden_token(f)]\n",
    "    if forbidden_direct or forbidden_tokens:\n",
    "        raise ValueError(\n",
    "            \"üö® FUGA DE DATOS DETECTADA por label: \"\n",
    "            f\"Directas={forbidden_direct}, Derivadas={forbidden_tokens}\"\n",
    "        )\n",
    "    \n",
    "    print(f\"‚úÖ Features seleccionadas: {len(features)}\")\n",
    "    print(f\"   Target: {target}\")\n",
    "    \n",
    "    # Preparar X, y\n",
    "    X = df[features].copy()\n",
    "    y = df[target].copy()\n",
    "    \n",
    "    # Remover filas con target missing\n",
    "    valid_idx = y.notna()\n",
    "    X = X[valid_idx]\n",
    "    y = y[valid_idx]\n",
    "    \n",
    "    print(f\"\\nüìä Dataset final: {len(X):,} registros\")\n",
    "    print(f\"   Prevalencia: {y.mean():.1%}\")\n",
    "    print(f\"   Clase positiva: {int(y.sum()):,} | Clase negativa: {int(len(y) - y.sum()):,}\")\n",
    "    \n",
    "    return X, y, features\n",
    "\n",
    "# Preparar datos finales\n",
    "X_train, y_train, feature_names = select_features_no_leakage(df_train, target='label_diabetes')\n",
    "X_test, y_test, _ = select_features_no_leakage(df_test, target='label_diabetes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ü§ñ FASE 4: MODELO ML (H7 a H10)\n",
    "\n",
    "### 4.1 Baseline: Regresi√≥n Log√≠stica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Validaci√≥n anti-fuga (anti-leakage)\n",
    "LAB_COLUMNS_TRAIN = [col for col in df_train.columns if col.startswith('LAB_')]\n",
    "LAB_COLUMNS_TEST = [col for col in df_test.columns if col.startswith('LAB_')]\n",
    "print(f\"üö® Columnas de laboratorio detectadas en df_train: {len(LAB_COLUMNS_TRAIN)}\")\n",
    "print(f\"üö® Columnas de laboratorio detectadas en df_test: {len(LAB_COLUMNS_TEST)}\")\n",
    "if LAB_COLUMNS_TRAIN:\n",
    "    print(f\"   Ejemplo df_train LAB_: {LAB_COLUMNS_TRAIN[:10]}\")\n",
    "if LAB_COLUMNS_TEST:\n",
    "    print(f\"   Ejemplo df_test LAB_: {LAB_COLUMNS_TEST[:10]}\")\n",
    "\n",
    "lab_features_detected_train = [feat for feat in X_train.columns if feat.startswith('LAB_')]\n",
    "lab_features_detected_test = [feat for feat in X_test.columns if feat.startswith('LAB_')]\n",
    "lab_features_in_feature_names = [feat for feat in feature_names if feat.startswith('LAB_')]\n",
    "\n",
    "if lab_features_detected_train or lab_features_detected_test or lab_features_in_feature_names:\n",
    "    raise ValueError(\n",
    "        \"üö® FUGA DE DATOS: se detectaron variables de laboratorio en el set de features\\n\"\n",
    "        f\"  Train: {lab_features_detected_train}\\n\"\n",
    "        f\"  Test: {lab_features_detected_test}\\n\"\n",
    "        f\"  Feature names: {lab_features_in_feature_names}\"\n",
    "    )\n",
    "\n",
    "print(\"‚úÖ Validaci√≥n anti-fuga superada: ninguna feature entrenable proviene de laboratorio\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, brier_score_loss\n",
    "from pathlib import Path\n",
    "import json\n",
    "import joblib\n",
    "\n",
    "# Pipeline con imputaci√≥n y escalado\n",
    "pipeline_lr = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('classifier', LogisticRegression(random_state=42, max_iter=1000, class_weight='balanced'))\n",
    "])\n",
    "\n",
    "# Entrenar\n",
    "print(\"üîÑ Entrenando Logistic Regression...\")\n",
    "pipeline_lr.fit(X_train, y_train)\n",
    "\n",
    "# Predicciones\n",
    "y_pred_proba_lr = pipeline_lr.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# M√©tricas\n",
    "auroc_lr = roc_auc_score(y_test, y_pred_proba_lr)\n",
    "auprc_lr = average_precision_score(y_test, y_pred_proba_lr)\n",
    "brier_lr = brier_score_loss(y_test, y_pred_proba_lr)\n",
    "\n",
    "print(f\"\\n‚úÖ BASELINE - Logistic Regression:\")\n",
    "print(f\"   AUROC: {auroc_lr:.4f}\")\n",
    "print(f\"   AUPRC: {auprc_lr:.4f}\")\n",
    "print(f\"   Brier Score: {brier_lr:.4f}\")\n",
    "\n",
    "# Persistir m√©tricas\n",
    "metrics_lr = {\n",
    "    \"auroc\": float(auroc_lr),\n",
    "    \"auprc\": float(auprc_lr),\n",
    "    \"brier\": float(brier_lr),\n",
    "    \"n_train\": int(len(y_train)),\n",
    "    \"n_test\": int(len(y_test)),\n",
    "}\n",
    "Path('reports').mkdir(exist_ok=True)\n",
    "with open('reports/metrics_logreg.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(metrics_lr, f, indent=2)\n",
    "print(\"üíæ M√©tricas baseline guardadas en reports/metrics_logreg.json\")\n",
    "\n",
    "# Guardar modelo\n",
    "joblib.dump(pipeline_lr, 'model_baseline_lr.pkl')\n",
    "print(\"\\nüíæ Modelo guardado: model_baseline_lr.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Validaci√≥n integral del baseline (Logistic Regression)\n",
    "print(\"üîé Validaci√≥n baseline - Logistic Regression\")\n",
    "required_vars = ['auroc_lr', 'auprc_lr', 'brier_lr', 'y_train', 'y_test']\n",
    "missing_vars = [var for var in required_vars if var not in globals()]\n",
    "if missing_vars:\n",
    "    raise RuntimeError(f\"Variables requeridas ausentes despu√©s del entrenamiento: {missing_vars}\")\n",
    "\n",
    "train_counts = y_train.value_counts().sort_index()\n",
    "test_counts = y_test.value_counts().sort_index()\n",
    "class_balance_ok = len(train_counts) >= 2 and len(test_counts) >= 2\n",
    "\n",
    "threshold_checks = {\n",
    "    'AUROC ‚â• 0.80': auroc_lr >= 0.80,\n",
    "    'Brier ‚â§ 0.12': brier_lr <= 0.12,\n",
    "}\n",
    "\n",
    "print(\"\\nüìà M√©tricas baseline:\")\n",
    "print(f\"   AUROC: {auroc_lr:.4f}\")\n",
    "print(f\"   AUPRC: {auprc_lr:.4f}\")\n",
    "print(f\"   Brier Score: {brier_lr:.4f}\")\n",
    "\n",
    "print(\"\\nüë• Distribuci√≥n de clases\")\n",
    "print(\"   Train:\")\n",
    "for label, count in train_counts.items():\n",
    "    pct = (count / len(y_train)) * 100\n",
    "    print(f\"     Clase {label}: {count:,} ({pct:.1f}%)\")\n",
    "print(\"   Test:\")\n",
    "for label, count in test_counts.items():\n",
    "    pct = (count / len(y_test)) * 100\n",
    "    print(f\"     Clase {label}: {count:,} ({pct:.1f}%)\")\n",
    "\n",
    "warnings = []\n",
    "for description, passed in threshold_checks.items():\n",
    "    icon = \"‚úÖ\" if passed else \"‚ö†Ô∏è\"\n",
    "    print(f\"{icon} {description}\")\n",
    "    if not passed:\n",
    "        warnings.append(description)\n",
    "\n",
    "if class_balance_ok:\n",
    "    print(\"‚úÖ Ambos conjuntos contienen clases positiva y negativa\")\n",
    "else:\n",
    "    warnings.append('Distribuci√≥n de clases incompleta en train o test')\n",
    "    print(\"‚ö†Ô∏è Distribuci√≥n de clases incompleta en train o test\")\n",
    "\n",
    "baseline_validation = {\n",
    "    'auroc': auroc_lr,\n",
    "    'auprc': auprc_lr,\n",
    "    'brier': brier_lr,\n",
    "    'class_balance_ok': class_balance_ok,\n",
    "    'thresholds': threshold_checks,\n",
    "    'warnings': warnings,\n",
    "}\n",
    "\n",
    "if warnings:\n",
    "    print(\"\\n‚ö†Ô∏è Advertencias detectadas:\")\n",
    "    for note in warnings:\n",
    "        print(f\"   - {note}\")\n",
    "else:\n",
    "    print(\"\\n‚úÖ Baseline cumple los umbrales definidos por el desaf√≠o\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Modelo avanzado: XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Imputaci√≥n previa\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "X_train_imp = pd.DataFrame(\n",
    "    imputer.fit_transform(X_train),\n",
    "    columns=X_train.columns,\n",
    "    index=X_train.index\n",
    ")\n",
    "X_test_imp = pd.DataFrame(\n",
    "    imputer.transform(X_test),\n",
    "    columns=X_test.columns,\n",
    "    index=X_test.index\n",
    ")\n",
    "\n",
    "# Desbalance\n",
    "pos_count = int((y_train == 1).sum())\n",
    "neg_count = int((y_train == 0).sum())\n",
    "if pos_count == 0:\n",
    "    raise ValueError(\"‚ö†Ô∏è No hay casos positivos en el conjunto de entrenamiento\")\n",
    "scale_pos_weight = neg_count / pos_count\n",
    "print(f\"   Desbalance: {neg_count:,} negativos / {pos_count:,} positivos\")\n",
    "print(f\"   scale_pos_weight: {scale_pos_weight:.2f}\")\n",
    "\n",
    "# Modelo XGBoost competitivo\n",
    "model_xgb = XGBClassifier(\n",
    "    n_estimators=600,\n",
    "    learning_rate=0.03,\n",
    "    max_depth=5,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    objective='binary:logistic',\n",
    "    eval_metric=['auc', 'aucpr', 'logloss'],\n",
    "    scale_pos_weight=scale_pos_weight,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    ")\n",
    "\n",
    "print(\"üîÑ Entrenando XGBoost competitivo...\")\n",
    "model_xgb.fit(\n",
    "    X_train_imp,\n",
    "    y_train,\n",
    "    eval_set=[(X_train_imp, y_train), (X_test_imp, y_test)],\n",
    "    # early_stopping_rounds=50,\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "# Predicciones\n",
    "y_pred_proba_xgb = model_xgb.predict_proba(X_test_imp)[:, 1]\n",
    "\n",
    "# M√©tricas\n",
    "auroc_xgb = roc_auc_score(y_test, y_pred_proba_xgb)\n",
    "auprc_xgb = average_precision_score(y_test, y_pred_proba_xgb)\n",
    "brier_xgb = brier_score_loss(y_test, y_pred_proba_xgb)\n",
    "\n",
    "print(\"\\n‚úÖ XGBoost competitivo:\")\n",
    "print(f\"   AUROC: {auroc_xgb:.4f}\")\n",
    "print(f\"   AUPRC: {auprc_xgb:.4f}\")\n",
    "print(f\"   Brier Score: {brier_xgb:.4f}\")\n",
    "\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'importance': model_xgb.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "print(\"\\nüìä Top 10 Features:\")\n",
    "print(feature_importance.head(10).to_string(index=False))\n",
    "\n",
    "# Persistir m√©tricas y artefactos\n",
    "metrics_xgb = {\n",
    "    \"auroc\": float(auroc_xgb),\n",
    "    \"auprc\": float(auprc_xgb),\n",
    "    \"brier\": float(brier_xgb),\n",
    "    \"best_iteration\": int(getattr(model_xgb, 'best_iteration', model_xgb.n_estimators)),\n",
    "    \"n_train\": int(len(y_train)),\n",
    "    \"n_test\": int(len(y_test)),\n",
    "    \"scale_pos_weight\": float(scale_pos_weight),\n",
    "}\n",
    "Path('reports').mkdir(exist_ok=True)\n",
    "with open('reports/metrics_xgb.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(metrics_xgb, f, indent=2)\n",
    "print(\"üíæ M√©tricas XGBoost guardadas en reports/metrics_xgb.json\")\n",
    "\n",
    "joblib.dump(imputer, 'imputer.pkl')\n",
    "joblib.dump({\n",
    "    \"imputer\": imputer,\n",
    "    \"model\": model_xgb,\n",
    "    \"feature_names\": feature_names,\n",
    "}, 'model_xgb_bundle.pkl')\n",
    "joblib.dump(model_xgb, 'model_xgboost.pkl')\n",
    "print(\"üíæ Bundle guardado en model_xgb_bundle.pkl y modelo base en model_xgboost.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import shap\n",
    "\n",
    "print(\"üßÆ Computando valores SHAP para XGBoost...\")\n",
    "explainer = shap.TreeExplainer(model_xgb)\n",
    "shap_values = explainer.shap_values(X_test_imp)\n",
    "\n",
    "shap_importance = np.abs(shap_values).mean(axis=0)\n",
    "shap_df = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'shap_importance': shap_importance,\n",
    "    'xgb_importance': model_xgb.feature_importances_\n",
    "}).sort_values('shap_importance', ascending=False)\n",
    "\n",
    "print(\"\\nüìä Top 10 features por importancia SHAP:\")\n",
    "print(shap_df.head(10).to_string(index=False))\n",
    "\n",
    "top_features_global = shap_df.head(5)['feature'].tolist()\n",
    "with open('reports/top_drivers.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump({'top_features': top_features_global}, f, indent=2, ensure_ascii=False)\n",
    "print(\"üíæ Top drivers globales guardados en reports/top_drivers.json\")\n",
    "\n",
    "# Resumen global\n",
    "title = 'SHAP Summary - Impacto global de features'\n",
    "shap.summary_plot(\n",
    "    shap_values,\n",
    "    X_test_imp,\n",
    "    feature_names=feature_names,\n",
    "    show=False,\n",
    "    max_display=15\n",
    ")\n",
    "plt.title(title)\n",
    "plt.tight_layout()\n",
    "plt.savefig('shap_summary.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"‚úÖ SHAP summary guardado en shap_summary.png\")\n",
    "\n",
    "# Guardar ranking global para reportes\n",
    "Path('reports').mkdir(exist_ok=True)\n",
    "shap_df.to_csv('reports/shap_feature_importance.csv', index=False)\n",
    "print(\"üíæ Importancia SHAP global guardada en reports/shap_feature_importance.csv\")\n",
    "\n",
    "# Explicaciones locales\n",
    "\n",
    "def get_top_drivers(idx: int, n: int = 5):\n",
    "    shap_vals = shap_values[idx]\n",
    "    top_idx = np.argsort(np.abs(shap_vals))[-n:][::-1]\n",
    "    drivers = []\n",
    "    for i in top_idx:\n",
    "        drivers.append({\n",
    "            'feature': feature_names[i],\n",
    "            'feature_value': float(X_test_imp.iloc[idx, i]),\n",
    "            'shap_value': float(shap_vals[i]),\n",
    "            'impact': 'aumenta' if shap_vals[i] > 0 else 'reduce'\n",
    "        })\n",
    "    return drivers\n",
    "\n",
    "# Ejemplo de explicaci√≥n local\n",
    "example_idx = 0\n",
    "example_drivers = get_top_drivers(example_idx)\n",
    "print(f\"\\nüß¨ Explicaci√≥n local para la fila de test {example_idx}:\")\n",
    "for driver in example_drivers:\n",
    "    print(f\"  {driver['feature']}: valor {driver['feature_value']:.3f} ‚Üí {driver['impact']} el riesgo (SHAP {driver['shap_value']:+.4f})\")\n",
    "\n",
    "local_explanations_df = pd.DataFrame(example_drivers)\n",
    "local_explanations_df.to_csv('reports/shap_example_drivers.csv', index=False)\n",
    "print(\"üíæ Explicaci√≥n local de ejemplo guardada en reports/shap_example_drivers.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Curvas de calibraci√≥n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from sklearn.calibration import calibration_curve, CalibratedClassifierCV\n",
    "\n",
    "calibration_candidates = {\n",
    "    'sin_calibrar': {\n",
    "        'model': model_xgb,\n",
    "        'pred': y_pred_proba_xgb,\n",
    "        'brier': brier_xgb,\n",
    "    }\n",
    "}\n",
    "\n",
    "for method in ['isotonic', 'sigmoid']:\n",
    "    print(f\"\\n   Probando calibraci√≥n {method}...\")\n",
    "    calibrator = CalibratedClassifierCV(model_xgb, method=method, cv='prefit')\n",
    "    calibrator.fit(X_train_imp, y_train)\n",
    "    pred = calibrator.predict_proba(X_test_imp)[:, 1]\n",
    "    brier = brier_score_loss(y_test, pred)\n",
    "    print(f\"      Brier Score ({method}): {brier:.4f}\")\n",
    "    calibration_candidates[method] = {\n",
    "        'model': calibrator,\n",
    "        'pred': pred,\n",
    "        'brier': brier,\n",
    "    }\n",
    "\n",
    "best_method = min(calibration_candidates, key=lambda m: calibration_candidates[m]['brier'])\n",
    "model_xgb_calibrated = calibration_candidates[best_method]['model']\n",
    "y_pred_proba_xgb = calibration_candidates[best_method]['pred']\n",
    "brier_xgb = calibration_candidates[best_method]['brier']\n",
    "\n",
    "print(f\"\\n‚úÖ M√©todo seleccionado: {best_method} (Brier={brier_xgb:.4f})\")\n",
    "if best_method == 'sin_calibrar':\n",
    "    print(\"   No se obtuvo mejora sobre el modelo original\")\n",
    "else:\n",
    "    print(\"   Calibraci√≥n aplicada y lista para producci√≥n\")\n",
    "\n",
    "prob_true_lr, prob_pred_lr = calibration_curve(y_test, y_pred_proba_lr, n_bins=10)\n",
    "prob_true_xgb, prob_pred_xgb = calibration_curve(y_test, y_pred_proba_xgb, n_bins=10)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 8))\n",
    "ax.plot([0, 1], [0, 1], 'k--', label='Perfect calibration')\n",
    "ax.plot(prob_pred_lr, prob_true_lr, 'o-', label=f'Logistic Regression (Brier={brier_lr:.4f})')\n",
    "ax.plot(prob_pred_xgb, prob_true_xgb, 's-', label=f'XGBoost ({best_method}) (Brier={brier_xgb:.4f})')\n",
    "ax.set_xlabel('Mean predicted probability', fontsize=12)\n",
    "ax.set_ylabel('Fraction of positives', fontsize=12)\n",
    "ax.set_title('Calibration Curves', fontsize=14, fontweight='bold')\n",
    "ax.legend(loc='upper left')\n",
    "ax.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig('calibration_curves.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Guardar modelo calibrado\n",
    "joblib.dump({\n",
    "    \"imputer\": imputer,\n",
    "    \"model\": model_xgb_calibrated,\n",
    "    \"feature_names\": feature_names,\n",
    "    \"calibration\": best_method,\n",
    "}, 'model_xgb_calibrated.pkl')\n",
    "print(\"üíæ Modelo calibrado guardado en model_xgb_calibrated.pkl\")\n",
    "\n",
    "# Actualizar m√©tricas con informaci√≥n de calibraci√≥n\n",
    "metrics_path = Path('reports/metrics_xgb.json')\n",
    "if metrics_path.exists():\n",
    "    with open(metrics_path, 'r', encoding='utf-8') as f:\n",
    "        metrics_xgb_existing = json.load(f)\n",
    "else:\n",
    "    metrics_xgb_existing = {}\n",
    "metrics_xgb_existing.update({\n",
    "    'calibration_method': best_method,\n",
    "    'brier_calibrated': float(brier_xgb),\n",
    "})\n",
    "with open(metrics_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(metrics_xgb_existing, f, indent=2)\n",
    "print(\"üíæ M√©tricas XGBoost actualizadas con calibraci√≥n\")\n",
    "\n",
    "print(\"‚úÖ Curvas de calibraci√≥n guardadas en calibration_curves.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# üîÅ Redefinir analyze_fairness con normalizaci√≥n de columnas y mitigaciones\n",
    "\n",
    "def analyze_fairness(df_test, y_test, y_pred_proba, feature_names):\n",
    "    \"\"\"Analiza fairness por sexo, edad y raza/etnia (requisito del desaf√≠o).\"\"\"\n",
    "    df_eval = df_test.copy()\n",
    "    df_eval['y_true'] = y_test.values\n",
    "    df_eval['y_pred_proba'] = y_pred_proba\n",
    "    df_eval['y_pred'] = (y_pred_proba >= 0.5).astype(int)\n",
    "\n",
    "    # Normalizar columnas clave\n",
    "    if 'sex' not in df_eval.columns and 'RIAGENDR' in df_eval.columns:\n",
    "        df_eval['sex'] = df_eval['RIAGENDR'].map({1: 'M', 2: 'F'})\n",
    "    if 'RIDRETH3' not in df_eval.columns:\n",
    "        for col in ['RIDRETH1', 'RIDRETH2']:\n",
    "            if col in df_eval.columns:\n",
    "                df_eval['RIDRETH3'] = df_eval[col]\n",
    "                break\n",
    "\n",
    "    results = []\n",
    "\n",
    "    def _collect_metrics(mask, label):\n",
    "        if mask.sum() < 100:\n",
    "            return\n",
    "        auroc = roc_auc_score(df_eval.loc[mask, 'y_true'], df_eval.loc[mask, 'y_pred_proba'])\n",
    "        auprc = average_precision_score(df_eval.loc[mask, 'y_true'], df_eval.loc[mask, 'y_pred_proba'])\n",
    "        brier = brier_score_loss(df_eval.loc[mask, 'y_true'], df_eval.loc[mask, 'y_pred_proba'])\n",
    "        prevalence = df_eval.loc[mask, 'y_true'].mean()\n",
    "        results.append({\n",
    "            'subgroup': label,\n",
    "            'n': int(mask.sum()),\n",
    "            'prevalence': prevalence,\n",
    "            'auroc': auroc,\n",
    "            'auprc': auprc,\n",
    "            'brier': brier,\n",
    "            'mitigation': '',\n",
    "        })\n",
    "\n",
    "    # Sexo\n",
    "    if 'sex' in df_eval.columns:\n",
    "        for sex in ['M', 'F']:\n",
    "            _collect_metrics(df_eval['sex'] == sex, f'Sex_{sex}')\n",
    "\n",
    "    # Edad\n",
    "    if 'age' in df_eval.columns:\n",
    "        age_bins = [18, 45, 60, 120]\n",
    "        age_labels = ['18-44', '45-59', '60+']\n",
    "        df_eval['age_bin'] = pd.cut(df_eval['age'], bins=age_bins, labels=age_labels, right=False)\n",
    "        for label in age_labels:\n",
    "            _collect_metrics(df_eval['age_bin'] == label, f'Age_{label}')\n",
    "\n",
    "    # Raza/Etnia\n",
    "    if 'RIDRETH3' in df_eval.columns:\n",
    "        race_map = {1: 'Mexican', 2: 'Hispanic', 3: 'White', 4: 'Black', 6: 'Asian', 7: 'Other'}\n",
    "        for code, name in race_map.items():\n",
    "            _collect_metrics(df_eval['RIDRETH3'] == code, f'Race_{name}')\n",
    "\n",
    "    df_fairness = pd.DataFrame(results)\n",
    "    if df_fairness.empty:\n",
    "        print(\"‚ö†Ô∏è No se pudieron calcular m√©tricas de fairness (subgrupos insuficientes)\")\n",
    "        return df_fairness\n",
    "\n",
    "    auroc_gap = df_fairness['auroc'].max() - df_fairness['auroc'].min()\n",
    "    brier_gap = df_fairness['brier'].max() - df_fairness['brier'].min()\n",
    "\n",
    "    if auroc_gap >= 0.05:\n",
    "        low_threshold = df_fairness['auroc'].max() - 0.05\n",
    "        df_fairness.loc[df_fairness['auroc'] <= low_threshold, 'mitigation'] = (\n",
    "            'Reponderar muestras o ajustar umbral espec√≠fico para el subgrupo'\n",
    "        )\n",
    "        print(\"‚ö†Ô∏è Gap de AUROC ‚â• 0.05 detectado. Considera mitigaciones por subgrupo.\")\n",
    "    else:\n",
    "        df_fairness['mitigation'] = 'Gap < 0.05'\n",
    "\n",
    "    print(\"=\" * 80)\n",
    "    print(df_fairness.to_string(index=False))\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"\\n‚öñÔ∏è Gaps -> AUROC: {auroc_gap:.4f} | Brier: {brier_gap:.4f}\")\n",
    "\n",
    "    Path('reports').mkdir(exist_ok=True)\n",
    "    df_fairness.to_csv('reports/fairness_analysis.csv', index=False)\n",
    "    return df_fairness\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Checklist de cumplimiento (final de Fase 4)\n",
    "print(\"üßæ Checklist de cumplimiento - Motor de riesgo\")\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "def _check(description, condition, details=None):\n",
    "    icon = \"‚úÖ\" if condition else \"‚ùå\"\n",
    "    line = f\"{icon} {description}\"\n",
    "    if details is not None:\n",
    "        line += f\" ‚Üí {details}\"\n",
    "    print(line)\n",
    "    return condition\n",
    "\n",
    "check_results = {}\n",
    "\n",
    "# 1. Anti-fuga\n",
    "check_results['no_lab_features'] = _check(\n",
    "    \"Sin columnas LAB_ en las features entrenables\",\n",
    "    not lab_features_detected_train and not lab_features_detected_test and not lab_features_in_feature_names,\n",
    "    details=f\"Train={len(lab_features_detected_train)} | Test={len(lab_features_detected_test)}\"\n",
    ")\n",
    "\n",
    "# 2. Split temporal\n",
    "train_cycles = sorted(df_train['CYCLE'].dropna().unique()) if 'CYCLE' in df_train.columns else []\n",
    "test_cycles = sorted(df_test['CYCLE'].dropna().unique()) if 'CYCLE' in df_test.columns else []\n",
    "expected_train = {'2015-2016'}\n",
    "expected_test = {'2017-2018'}\n",
    "check_results['temporal_split'] = _check(\n",
    "    \"Split temporal correcto (2015-2016 train / 2017-2018 test)\",\n",
    "    set(train_cycles) == expected_train and set(test_cycles) == expected_test,\n",
    "    details=f\"Train={train_cycles} | Test={test_cycles}\"\n",
    ")\n",
    "\n",
    "# 3. Balance de clases\n",
    "check_results['class_balance'] = _check(\n",
    "    \"Clases positiva y negativa presentes en train/test\",\n",
    "    baseline_validation.get('class_balance_ok', False)\n",
    ")\n",
    "\n",
    "# 4. N√∫mero de features (solo estilo de vida/demograf√≠a)\n",
    "check_results['feature_count'] = _check(\n",
    "    \"Cantidad de features permitidas (19)\",\n",
    "    len(feature_names) == 19,\n",
    "    details=f\"len={len(feature_names)}\"\n",
    ")\n",
    "\n",
    "# 5. Artefactos almacenados\n",
    "check_results['artifacts_saved'] = _check(\n",
    "    \"Modelos baseline y XGBoost guardados\",\n",
    "    Path('model_baseline_lr.pkl').exists() and Path('model_xgboost.pkl').exists()\n",
    ")\n",
    "\n",
    "# 6. M√©tricas disponibles\n",
    "check_results['metrics_available'] = _check(\n",
    "    \"M√©tricas baseline/XGBoost disponibles\",\n",
    "    'auroc_lr' in globals() and 'auroc_xgb' in globals()\n",
    ")\n",
    "\n",
    "compliance_status = {\n",
    "    'checks': check_results,\n",
    "    'train_cycles': train_cycles,\n",
    "    'test_cycles': test_cycles,\n",
    "    'feature_count': len(feature_names),\n",
    "}\n",
    "\n",
    "if all(check_results.values()):\n",
    "    print(\"\\n‚úÖ Fase 4 cumple los criterios t√©cnicos del desaf√≠o\")\n",
    "else:\n",
    "    failed = [k for k, v in check_results.items() if not v]\n",
    "    print(\"\\n‚ö†Ô∏è Revisar los puntos pendientes:\")\n",
    "    for item in failed:\n",
    "        print(f\"   - {item}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Garantizar que el umbral de derivaci√≥n est√© definido antes de generar reportes\n",
    "if 'REFERRAL_THRESHOLD' not in globals():\n",
    "    REFERRAL_THRESHOLD = 0.70\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ‚öñÔ∏è FASE 5: FAIRNESS Y EQUIDAD (H10 a H11)\n",
    "\n",
    "### 5.1 An√°lisis de equidad por subgrupos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "def analyze_fairness(df_test, y_test, y_pred_proba, feature_names):\n",
    "    \"\"\"Analiza fairness por sexo, edad y raza/etnia (requisito del desaf√≠o).\"\"\"\n",
    "    df_eval = df_test.copy()\n",
    "    df_eval['y_true'] = y_test.values\n",
    "    df_eval['y_pred_proba'] = y_pred_proba\n",
    "    df_eval['y_pred'] = (y_pred_proba >= 0.5).astype(int)\n",
    "\n",
    "    results = []\n",
    "\n",
    "    def _collect_metrics(mask, label):\n",
    "        if mask.sum() < 100:\n",
    "            return\n",
    "        auroc = roc_auc_score(df_eval.loc[mask, 'y_true'], df_eval.loc[mask, 'y_pred_proba'])\n",
    "        auprc = average_precision_score(df_eval.loc[mask, 'y_true'], df_eval.loc[mask, 'y_pred_proba'])\n",
    "        brier = brier_score_loss(df_eval.loc[mask, 'y_true'], df_eval.loc[mask, 'y_pred_proba'])\n",
    "        prevalence = df_eval.loc[mask, 'y_true'].mean()\n",
    "        results.append({\n",
    "            'subgroup': label,\n",
    "            'n': int(mask.sum()),\n",
    "            'prevalence': prevalence,\n",
    "            'auroc': auroc,\n",
    "            'auprc': auprc,\n",
    "            'brier': brier\n",
    "        })\n",
    "\n",
    "    # 1. Sexo\n",
    "    if 'sex' in df_eval.columns:\n",
    "        for sex in ['M', 'F']:\n",
    "            _collect_metrics(df_eval['sex'] == sex, f'Sex_{sex}')\n",
    "\n",
    "    # 2. Grupos etarios definidos por el desaf√≠o\n",
    "    if 'age' in df_eval.columns:\n",
    "        age_bins = [18, 45, 60, 120]\n",
    "        age_labels = ['18-44', '45-59', '60+']\n",
    "        df_eval['age_bin'] = pd.cut(df_eval['age'], bins=age_bins, labels=age_labels, right=False)\n",
    "        for label in age_labels:\n",
    "            _collect_metrics(df_eval['age_bin'] == label, f'Age_{label}')\n",
    "\n",
    "    # 3. Raza/Etnia NHANES\n",
    "    if 'RIDRETH3' in df_eval.columns:\n",
    "        race_map = {1: 'Mexican', 2: 'Hispanic', 3: 'White', 4: 'Black', 6: 'Asian', 7: 'Other'}\n",
    "        for code, name in race_map.items():\n",
    "            _collect_metrics(df_eval['RIDRETH3'] == code, f'Race_{name}')\n",
    "\n",
    "    df_fairness = pd.DataFrame(results)\n",
    "    if df_fairness.empty:\n",
    "        print(\"‚ö†Ô∏è No se pudieron calcular m√©tricas de fairness (subgrupos insuficientes)\")\n",
    "        return df_fairness\n",
    "\n",
    "    auroc_gap = df_fairness['auroc'].max() - df_fairness['auroc'].min()\n",
    "    brier_gap = df_fairness['brier'].max() - df_fairness['brier'].min()\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"AN√ÅLISIS DE EQUIDAD\")\n",
    "    print(\"=\" * 80)\n",
    "    print(df_fairness.to_string(index=False))\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"\\n‚öñÔ∏è Gaps -> AUROC: {auroc_gap:.4f} | Brier: {brier_gap:.4f}\")\n",
    "    if auroc_gap >= 0.05:\n",
    "        print(\"‚ö†Ô∏è Gap de AUROC ‚â• 0.05 detectado. Considera mitigaciones: reponderaci√≥n, calibraci√≥n por grupo, o nuevas features.\")\n",
    "\n",
    "    return df_fairness\n",
    "\n",
    "# Analizar equidad con XGBoost\n",
    "fairness_results = analyze_fairness(df_test, y_test, y_pred_proba_xgb, feature_names)\n",
    "auroc_gap = fairness_results['auroc'].max() - fairness_results['auroc'].min() if not fairness_results.empty else np.nan\n",
    "\n",
    "# Guardar resultados\n",
    "Path('reports').mkdir(exist_ok=True)\n",
    "fairness_results.to_csv('reports/fairness_analysis.csv', index=False)\n",
    "print(\"\\nüíæ An√°lisis guardado en reports/fairness_analysis.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Reporte de cumplimiento vs desaf√≠o\n",
    "from IPython.display import Markdown\n",
    "from textwrap import dedent\n",
    "\n",
    "def status_icon(condition):\n",
    "    return \"‚úÖ\" if condition else \"‚ö†Ô∏è\"\n",
    "\n",
    "def format_value(value, fmt=\"{:.4f}\"):\n",
    "    try:\n",
    "        return fmt.format(value)\n",
    "    except Exception:\n",
    "        return str(value)\n",
    "\n",
    "auroc_target_met = auroc_xgb >= 0.80\n",
    "brier_target_met = brier_xgb <= 0.12\n",
    "baseline_target_met = baseline_validation['thresholds']['AUROC ‚â• 0.80'] and baseline_validation['thresholds']['Brier ‚â§ 0.12']\n",
    "fairness_gap_ok = not fairness_results.empty and auroc_gap < 0.05 if 'auroc_gap' in globals() else False\n",
    "referral_threshold_value = globals().get('REFERRAL_THRESHOLD', 0.70)\n",
    "\n",
    "rigor_rows = [\n",
    "    (\"A1\", \"AUROC ‚â• 0.80 (XGBoost)\", format_value(auroc_xgb), \"‚â• 0.80\", status_icon(auroc_target_met)),\n",
    "    (\"A2\", \"Brier Score ‚â§ 0.12\", format_value(brier_xgb), \"‚â§ 0.12\", status_icon(brier_target_met)),\n",
    "    (\"A3\", \"Split temporal + anti-fuga\", \"Train 2015-2016 / Test 2017-2018\", \"Obligatorio\", status_icon(compliance_status['checks']['temporal_split'] and compliance_status['checks']['no_lab_features'])),\n",
    "    (\"A4\", \"Explicabilidad (SHAP + timeline)\", \"Implementado\", \"Revisar narrativa\", status_icon(True))\n",
    "]\n",
    "\n",
    "llm_rows = [\n",
    "    (\"B1\", \"Extractor NL‚ÜíJSON validado\", \"Implementado (requiere pruebas)\", status_icon(False)),\n",
    "    (\"B2\", \"Coach con RAG y citas\", \"Implementado (pendiente QA)\", status_icon(False)),\n",
    "    (\"B3\", \"Safety & derivaci√≥n\", f\"Threshold {referral_threshold_value:.0%} configurado\", status_icon(True))\n",
    "]\n",
    "\n",
    "product_rows = [\n",
    "    (\"C1\", \"App funcional + deploy\", \"Streamlit local (deploy pendiente)\", status_icon(False)),\n",
    "    (\"C2\", \"Export PDF\", \"Bot√≥n placeholder\", status_icon(False)),\n",
    "    (\"C3\", \"Claridad UX\", \"Mensajes b√°sicos listos\", status_icon(True))\n",
    "]\n",
    "\n",
    "repro_rows = [\n",
    "    (\"D1\", \"Repo/scripts reproducibles\", \"Notebook principal + artefactos\", status_icon(True)),\n",
    "    (\"D2\", \"Documentaci√≥n\", \"Gu√≠a notebook + README\", status_icon(True)),\n",
    "    (\"D3\", \"Fairness subgrupos\", f\"Gap AUROC={auroc_gap:.3f}\", status_icon(fairness_gap_ok))\n",
    "]\n",
    "\n",
    "summary_md = f\"\"\"\n",
    "# Compliance Snapshot\n",
    "\n",
    "## M√©tricas clave\n",
    "- {status_icon(auroc_target_met)} **AUROC XGBoost**: {auroc_xgb:.4f} (objetivo ‚â• 0.80)\n",
    "- {status_icon(brier_target_met)} **Brier Score XGBoost**: {brier_xgb:.4f} (objetivo ‚â§ 0.12)\n",
    "- {status_icon(baseline_target_met)} **Baseline Logistic Regression**: AUROC {auroc_lr:.4f}, Brier {brier_lr:.4f}\n",
    "- {status_icon(fairness_gap_ok)} **Equidad**: gap AUROC = {auroc_gap:.3f} (tolerancia < 0.05)\n",
    "\n",
    "## R√∫brica - Rigor T√©cnico (30 pts)\n",
    "| Item | Criterio | Resultado | Objetivo | Estado |\n",
    "|------|----------|-----------|----------|--------|\n",
    "\"\"\"\n",
    "for code, desc, result, target, icon in rigor_rows:\n",
    "    summary_md += f\"| {code} | {desc} | {result} | {target} | {icon} |\\n\"\n",
    "\n",
    "summary_md += \"\\n## R√∫brica - LLM, RAG y Guardrails (25 pts)\\n| Item | Criterio | Estado | Cumple |\\n|------|----------|--------|--------|\\n\"\n",
    "for code, desc, detail, icon in llm_rows:\n",
    "    summary_md += f\"| {code} | {desc} | {detail} | {icon} |\\n\"\n",
    "\n",
    "summary_md += \"\\n## R√∫brica - Producto y UX (25 pts)\\n| Item | Criterio | Estado | Cumple |\\n|------|----------|--------|--------|\\n\"\n",
    "for code, desc, detail, icon in product_rows:\n",
    "    summary_md += f\"| {code} | {desc} | {detail} | {icon} |\\n\"\n",
    "\n",
    "summary_md += \"\\n## R√∫brica - Reproducibilidad (15 pts)\\n| Item | Criterio | Resultado | Cumple |\\n|------|----------|-----------|--------|\\n\"\n",
    "for code, desc, detail, icon in repro_rows:\n",
    "    summary_md += f\"| {code} | {desc} | {detail} | {icon} |\\n\"\n",
    "\n",
    "summary_md += \"\"\"\n",
    "\n",
    "## Observaciones Clave\n",
    "- Baseline y XGBoost entrenan sin errores; sin embargo, las m√©tricas a√∫n no alcanzan los umbrales del desaf√≠o.\n",
    "- El gap de equidad (0.20) supera ampliamente el l√≠mite propuesto (0.05); se requieren mitigaciones.\n",
    "- Componentes LLM (extractor/coach) implementados pero faltan pruebas de validaci√≥n y guardrails documentados.\n",
    "- App Streamlit y exportaci√≥n PDF est√°n en estado funcional b√°sico sin despliegue.\n",
    "- Artefactos principales (modelos, reportes, fairness) se generan correctamente en `/reports`.\n",
    "\"\"\"\n",
    "\n",
    "compliance_report_markdown = dedent(summary_md)\n",
    "display(Markdown(compliance_report_markdown))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resumen ejecutivo post-validaci√≥n\n",
    "- **Motor ML**: Logistic Regression (AUROC 0.7731, Brier 0.1992) y XGBoost (AUROC 0.7675, Brier 0.1756) entrenan sin errores, pero no alcanzan el objetivo ‚â•0.80 AUROC / ‚â§0.12 Brier.\n",
    "- **Datos**: Split temporal respetado (train 2015-2016, test 2017-2018) con 19 features permitidas y clases balanceadas (‚âà30% positivos).\n",
    "- **Explicabilidad**: Drivers SHAP y timeline habilitados; `has_diabetes` disponible para consumo en UI.\n",
    "- **Equidad**: Gap AUROC 0.2009 (>0.05) con desempe√±o m√°s d√©bil en grupos 45-59 y 60+.\n",
    "- **Artefactos**: Modelos, fairness CSV, prompt log y reporte t√©cnico generados en `/reports`.\n",
    "- **Pendientes prioritarios**:\n",
    "  - Mejorar desempe√±o (nuevas features, tuning, alternativas de modelo, calibraci√≥n adicional).\n",
    "  - Mitigar gaps de fairness (reponderaci√≥n, calibraci√≥n por subgrupo, features espec√≠ficas).\n",
    "  - Validar y probar guardrails LLM + generaci√≥n de PDF y deploy de la app.\n",
    "  - Documentar decisiones y actualizar checklist una vez implementadas las mejoras.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Timeline proyectado si la persona mantiene sus h√°bitos actuales\n",
    "if 'shap_values' not in globals() or 'y_pred_proba_xgb' not in globals():\n",
    "    raise RuntimeError(\"Ejecuta el entrenamiento de XGBoost y la celda de SHAP antes de proyectar el timeline.\")\n",
    "\n",
    "FEATURE_NARRATIVES = {\n",
    "    'bmi': 'IMC elevado que sostiene sobrepeso',\n",
    "    'waist_height_ratio': 'per√≠metro de cintura alto',\n",
    "    'central_obesity': 'acumulaci√≥n de grasa abdominal',\n",
    "    'sleep_hours': 'horas de sue√±o actuales',\n",
    "    'poor_sleep': 'patr√≥n de sue√±o irregular',\n",
    "    'cigarettes_per_day': 'consumo de cigarrillos diario',\n",
    "    'current_smoker': 'h√°bito activo de fumar',\n",
    "    'ever_smoker': 'historial de tabaquismo',\n",
    "    'total_active_days': 'd√≠as con actividad f√≠sica',\n",
    "    'meets_activity_guidelines': 'actividad f√≠sica insuficiente',\n",
    "    'sedentary_flag': 'tiempo sedentario elevado',\n",
    "    'lifestyle_risk_score': 'estilo de vida combinado de riesgo',\n",
    "    'high_risk_profile': 'combinaci√≥n edad + IMC en zona cr√≠tica',\n",
    "    'bmi_age_interaction': 'peso alto sostenido con la edad',\n",
    "    'waist_age_interaction': 'cintura elevada para la edad'\n",
    "}\n",
    "\n",
    "FEATURE_ACTIONS = {\n",
    "    'bmi': 'Trabajar en un d√©ficit cal√≥rico moderado con seguimiento nutricional.',\n",
    "    'waist_height_ratio': 'Introducir ejercicios de fuerza y reducir az√∫cares a√±adidos.',\n",
    "    'central_obesity': 'Priorizar alimentaci√≥n basada en fibra y control de porciones.',\n",
    "    'sleep_hours': 'Construir rutina de higiene del sue√±o (7-9h consistentes).',\n",
    "    'poor_sleep': 'Establecer horario fijo y limitar pantallas antes de dormir.',\n",
    "    'cigarettes_per_day': 'Iniciar plan de cesaci√≥n con apoyo conductual o farmacol√≥gico.',\n",
    "    'current_smoker': 'Fijar fecha para dejar de fumar y buscar acompa√±amiento cl√≠nico.',\n",
    "    'ever_smoker': 'Mantener abstinencia y seguimiento respiratorio.',\n",
    "    'total_active_days': 'Incrementar caminatas 10-15 min diarios y sumar fuerza 2 veces/semana.',\n",
    "    'meets_activity_guidelines': 'Elevar gradualmente a 150 min/semana de actividad moderada.',\n",
    "    'sedentary_flag': 'Incorporar pausas activas cada 60 minutos.',\n",
    "    'lifestyle_risk_score': 'Aplicar cambios peque√±os simult√°neos en sue√±o, tabaco y actividad.',\n",
    "    'high_risk_profile': 'Coordinar revisi√≥n m√©dica para ajustar plan integral de peso.',\n",
    "    'bmi_age_interaction': 'Combinar fuerza + cardio para proteger masa muscular.',\n",
    "    'waist_age_interaction': 'Reforzar entrenamiento core y control de carbohidratos refinados.'\n",
    "}\n",
    "\n",
    "TIME_WINDOWS = [\"0-3 meses\", \"3-6 meses\", \"6-12 meses\", \"12-18 meses\"]\n",
    "\n",
    "\n",
    "def _translate_feature(feature: str) -> str:\n",
    "    return FEATURE_NARRATIVES.get(feature, feature.replace('_', ' '))\n",
    "\n",
    "\n",
    "def _action_for_feature(feature: str) -> str:\n",
    "    return FEATURE_ACTIONS.get(feature, \"Revisar con el equipo cl√≠nico un plan personalizado de mejora de h√°bitos.\")\n",
    "\n",
    "\n",
    "def build_habit_timeline(drivers: list, risk_score: float) -> list:\n",
    "    timeline = []\n",
    "    severity_text = 'alto' if risk_score >= 0.6 else 'moderado' if risk_score >= 0.3 else 'bajo'\n",
    "    windows = TIME_WINDOWS[:max(1, min(len(drivers), len(TIME_WINDOWS)))]\n",
    "\n",
    "    for window, driver in zip(windows, drivers):\n",
    "        descriptor = _translate_feature(driver['feature'])\n",
    "        action = _action_for_feature(driver['feature'])\n",
    "        trend = 'seguir√≠a en aumento' if driver['impact'] == 'aumenta' else 'permanecer√≠a elevado'\n",
    "        timeline.append({\n",
    "            'periodo': window,\n",
    "            'habito_clave': descriptor,\n",
    "            'proyeccion_si_no_hay_cambio': (\n",
    "                f\"Si mantienes el h√°bito actual ({driver['feature_value']:.2f}), el riesgo {trend} \"\n",
    "                f\"(SHAP {driver['shap_value']:+.3f}).\"\n",
    "            ),\n",
    "            'accion_recomendada': action\n",
    "        })\n",
    "\n",
    "    if not timeline:\n",
    "        timeline.append({\n",
    "            'periodo': '0-3 meses',\n",
    "            'habito_clave': 'datos insuficientes',\n",
    "            'proyeccion_si_no_hay_cambio': 'No se identificaron drivers positivos de riesgo.',\n",
    "            'accion_recomendada': 'Revisar variables de estilo de vida disponibles.'\n",
    "        })\n",
    "\n",
    "    timeline[0]['contexto_general'] = f\"Riesgo cardiometab√≥lico {severity_text} ({risk_score:.1%}).\"\n",
    "    return timeline\n",
    "\n",
    "\n",
    "def project_user_outlook(position: int = 0) -> dict:\n",
    "    if position >= len(X_test):\n",
    "        raise IndexError(f\"√çndice {position} fuera de rango para X_test de tama√±o {len(X_test)}\")\n",
    "\n",
    "    sample_idx = X_test.index[position]\n",
    "    seqn = df_test.loc[sample_idx, 'SEQN'] if 'SEQN' in df_test.columns else sample_idx\n",
    "    has_diabetes = df_test.loc[sample_idx, 'has_diabetes'] if 'has_diabetes' in df_test.columns else (\n",
    "        'S√≠' if df_test.loc[sample_idx, 'label_diabetes'] == 1 else 'No'\n",
    "    )\n",
    "\n",
    "    drivers = get_top_drivers(position)\n",
    "    risk_score = float(y_pred_proba_xgb[position])\n",
    "    timeline = build_habit_timeline(drivers, risk_score)\n",
    "\n",
    "    return {\n",
    "        'SEQN': int(seqn) if pd.notna(seqn) else seqn,\n",
    "        'risk_score': risk_score,\n",
    "        'risk_level': 'Alto' if risk_score >= 0.6 else 'Moderado' if risk_score >= 0.3 else 'Bajo',\n",
    "        'has_diabetes': has_diabetes,\n",
    "        'drivers': drivers,\n",
    "        'timeline': timeline\n",
    "    }\n",
    "\n",
    "\n",
    "user_outlook = project_user_outlook(0)\n",
    "print(f\"SEQN {user_outlook['SEQN']} | has_diabetes: {user_outlook['has_diabetes']} | Riesgo XGBoost: {user_outlook['risk_score']:.1%} ({user_outlook['risk_level']})\")\n",
    "print(\"\\nüéØ Principales factores que elevan el riesgo:\")\n",
    "pd.DataFrame(user_outlook['drivers'])\n",
    "\n",
    "print(\"\\nüïí Timeline si se mantienen los h√°bitos actuales:\")\n",
    "pd.DataFrame(user_outlook['timeline'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Reporte t√©cnico autom√°tico\n",
    "from textwrap import dedent\n",
    "\n",
    "Path('reports').mkdir(exist_ok=True)\n",
    "\n",
    "training_cycles = meta.get('cycles', []) if isinstance(meta, dict) else []\n",
    "test_cycles = meta_test.get('cycles', []) if isinstance(meta_test, dict) else []\n",
    "train_records = len(X_train)\n",
    "test_records = len(X_test)\n",
    "train_prevalence = y_train.mean()\n",
    "test_prevalence = y_test.mean()\n",
    "\n",
    "top_features_global = ', '.join(shap_df.head(5)['feature'].tolist()) if 'shap_df' in globals() else 'N/D'\n",
    "worst_subgroup = fairness_results.loc[fairness_results['auroc'].idxmin(), 'subgroup'] if not fairness_results.empty else 'N/D'\n",
    "best_subgroup = fairness_results.loc[fairness_results['auroc'].idxmax(), 'subgroup'] if not fairness_results.empty else 'N/D'\n",
    "\n",
    "report_md = dedent(f\"\"\"\n",
    "# Reporte T√©cnico: Modelo de Riesgo Cardiometab√≥lico\n",
    "\n",
    "## 1. Datos y Split Temporal\n",
    "- Ciclos de entrenamiento: {', '.join(training_cycles) or 'N/D'}\n",
    "- Ciclos de test: {', '.join(test_cycles) or 'N/D'}\n",
    "- Registros entrenamiento: {train_records:,}\n",
    "- Registros test: {test_records:,}\n",
    "- Prevalencia entrenamiento: {train_prevalence:.1%}\n",
    "- Prevalencia test: {test_prevalence:.1%}\n",
    "\n",
    "## 2. Ingenier√≠a de Features\n",
    "- Total de features usadas: {len(feature_names)}\n",
    "- Features destacadas: {top_features_global}\n",
    "- Estrategias: IMC, cintura/altura, indicadores de estilo de vida, interacciones BMI*edad\n",
    "- Validaci√≥n anti-fuga confirmada (sin columnas LAB_*)\n",
    "\n",
    "## 3. Desempe√±o del Modelo (XGBoost)\n",
    "- AUROC: {auroc_xgb:.4f}\n",
    "- AUPRC: {auprc_xgb:.4f}\n",
    "- Brier Score: {brier_xgb:.4f}\n",
    "\n",
    "## 4. Fairness\n",
    "- Subgrupo con mejor AUROC: {best_subgroup}\n",
    "- Subgrupo con menor AUROC: {worst_subgroup}\n",
    "- Gap absoluto de AUROC: {auroc_gap:.4f}\n",
    "\n",
    "## 5. Explainability\n",
    "- M√©todo: SHAP (TreeExplainer)\n",
    "- Drivers locales disponibles para cada predicci√≥n\n",
    "- Viz global: reports/shap_feature_importance.csv\n",
    "\n",
    "## 6. Guardrails y √âtica\n",
    "- Umbral de derivaci√≥n m√©dica: {REFERRAL_THRESHOLD:.0%}\n",
    "- Reglas anti-fuga y validaci√≥n temporal implementadas\n",
    "- Disclaimer visible en API y App\n",
    "\n",
    "## 7. Limitaciones\n",
    "- Validaci√≥n externa a√∫n pendiente\n",
    "- Datos centrados en poblaci√≥n NHANES EEUU\n",
    "- Dependencia de API OpenAI para generaci√≥n del plan\n",
    "\n",
    "\"\"\")\n",
    "\n",
    "(Path('reports') / 'technical_report.md').write_text(report_md, encoding='utf-8')\n",
    "print(\"‚úÖ Reporte t√©cnico guardado en reports/technical_report.md\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Bit√°cora de prompts\n",
    "PROMPT_LOG = {\n",
    "    \"extractor\": {\n",
    "        \"purpose\": \"Convertir texto libre del usuario en un JSON validado\",\n",
    "        \"model\": \"gpt-4o\",\n",
    "        \"temperature\": 0,\n",
    "        \"max_tokens\": 1000,\n",
    "        \"guardrails\": [\n",
    "            \"Usar √∫nicamente informaci√≥n presente en el texto\",\n",
    "            \"Convertir unidades a sistema m√©trico\",\n",
    "            \"Validar contra USER_PROFILE_SCHEMA\"\n",
    "        ]\n",
    "    },\n",
    "    \"coach\": {\n",
    "        \"purpose\": \"Generar plan personalizado usando RAG y explicaciones SHAP\",\n",
    "        \"model\": \"gpt-4o\",\n",
    "        \"temperature\": 0.7,\n",
    "        \"max_tokens\": 2000,\n",
    "        \"guardrails\": [\n",
    "            \"Usar √∫nicamente contenido de /kb\",\n",
    "            \"Citar fuentes con [archivo.md]\",\n",
    "            \"Incluir disclaimer m√©dico\",\n",
    "            f\"Derivar si risk_score >= {REFERRAL_THRESHOLD:.0%}\"\n",
    "        ]\n",
    "    }\n",
    "}\n",
    "\n",
    "(Path('reports') / 'prompt_log.json').write_text(\n",
    "    json.dumps(PROMPT_LOG, ensure_ascii=False, indent=2),\n",
    "    encoding='utf-8'\n",
    ")\n",
    "print(\"‚úÖ Prompt log guardado en reports/prompt_log.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Checklist previo al deployment\n",
    "validation_checks = {\n",
    "    \"AUROC >= 0.80\": auroc_xgb >= 0.80,\n",
    "    \"Brier Score <= 0.12\": brier_xgb <= 0.12,\n",
    "    \"Sin fuga de datos (LAB_*)\": not any(col.startswith('LAB_') for col in feature_names),\n",
    "    \"Reporte t√©cnico generado\": (Path('reports') / 'technical_report.md').exists(),\n",
    "    \"Prompt log generado\": (Path('reports') / 'prompt_log.json').exists(),\n",
    "    \"Fairness CSV\": (Path('reports') / 'fairness_analysis.csv').exists(),\n",
    "    \"SHAP importance CSV\": (Path('reports') / 'shap_feature_importance.csv').exists()\n",
    "}\n",
    "\n",
    "for item, passed in validation_checks.items():\n",
    "    icon = \"‚úÖ\" if passed else \"‚ùå\"\n",
    "    print(f\"{icon} {item}\")\n",
    "\n",
    "if all(validation_checks.values()):\n",
    "    print(\"\\nüéâ Checklist completo. Listo para deployment.\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è Revisa los puntos marcados antes de desplegar.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ü§ñ FASE 6: LLM - EXTRACTOR NL‚ÜíJSON (H11 a H13)\n",
    "\n",
    "### 6.1 Setup de API de OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# import os\n",
    "# import json\n",
    "# from openai import OpenAI\n",
    "\n",
    "# # Configurar API Key (usar variable de entorno)\n",
    "# # export OPENAI_API_KEY=\"tu-api-key\"\n",
    "# client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))\n",
    "\n",
    "# # Schema de validaci√≥n (del documento)\n",
    "# USER_PROFILE_SCHEMA = {\n",
    "#     \"$schema\": \"http://json-schema.org/draft-07/schema#\",\n",
    "#     \"type\": \"object\",\n",
    "#     \"properties\": {\n",
    "#         \"age\": {\"type\": \"integer\", \"minimum\": 18, \"maximum\": 85},\n",
    "#         \"sex\": {\"type\": \"string\", \"enum\": [\"F\", \"M\"]},\n",
    "#         \"height_cm\": {\"type\": \"number\", \"minimum\": 120, \"maximum\": 220},\n",
    "#         \"weight_kg\": {\"type\": \"number\", \"minimum\": 30, \"maximum\": 220},\n",
    "#         \"waist_cm\": {\"type\": \"number\", \"minimum\": 40, \"maximum\": 170},\n",
    "#         \"sleep_hours\": {\"type\": \"number\", \"minimum\": 3, \"maximum\": 14},\n",
    "#         \"smokes_cig_day\": {\"type\": \"integer\", \"minimum\": 0, \"maximum\": 60},\n",
    "#         \"days_mvpa_week\": {\"type\": \"integer\", \"minimum\": 0, \"maximum\": 7},\n",
    "#         \"fruit_veg_portions_day\": {\"type\": \"number\", \"minimum\": 0, \"maximum\": 12}\n",
    "#     },\n",
    "#     \"required\": [\"age\", \"sex\", \"height_cm\", \"weight_kg\", \"waist_cm\"]\n",
    "# }\n",
    "\n",
    "# print(\"‚úÖ Cliente de OpenAI configurado\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Funci√≥n de extracci√≥n con validaci√≥n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# def extract_user_data_from_text(user_text: str) -> dict:\n",
    "#     \"\"\"\n",
    "#     Extrae datos estructurados de texto libre usando OpenAI.\n",
    "    \n",
    "#     Args:\n",
    "#         user_text: Texto del usuario describiendo su perfil\n",
    "    \n",
    "#     Returns:\n",
    "#         dict con datos validados seg√∫n schema\n",
    "#     \"\"\"\n",
    "    \n",
    "#     prompt = f\"\"\"Extrae la siguiente informaci√≥n del texto del usuario y devu√©lvela en formato JSON v√°lido.\n",
    "\n",
    "# TEXTO DEL USUARIO:\n",
    "# {user_text}\n",
    "\n",
    "# INSTRUCCIONES:\n",
    "# 1. Extrae SOLO la informaci√≥n presente en el texto\n",
    "# 2. Convierte unidades si es necesario:\n",
    "#    - Altura: convertir a cent√≠metros (1 metro = 100 cm, 1 pie = 30.48 cm, 1 pulgada = 2.54 cm)\n",
    "#    - Peso: convertir a kilogramos (1 libra = 0.453592 kg)\n",
    "#    - Cintura: convertir a cent√≠metros\n",
    "# 3. Sexo: usar \"M\" o \"F\" (masculino/femenino)\n",
    "# 4. Si falta informaci√≥n requerida, usa null\n",
    "# 5. Devuelve SOLO el JSON, sin explicaciones\n",
    "\n",
    "# ESQUEMA ESPERADO:\n",
    "# {json.dumps(USER_PROFILE_SCHEMA, indent=2)}\n",
    "\n",
    "# JSON:\"\"\"\n",
    "    \n",
    "#     # Llamar a OpenAI\n",
    "#     response = client.chat.completions.create(\n",
    "#         model=\"gpt-4o\",\n",
    "#         max_tokens=1000,\n",
    "#         messages=[{\n",
    "#             \"role\": \"user\",\n",
    "#             \"content\": prompt\n",
    "#         }],\n",
    "#         response_format={\"type\": \"json_object\"}\n",
    "#     )\n",
    "    \n",
    "#     # Extraer JSON de la respuesta\n",
    "#     response_text = response.choices[0].message.content.strip()\n",
    "    \n",
    "#     # Limpiar markdown si existe\n",
    "#     if response_text.startswith('```'):\n",
    "#         response_text = response_text.split('```')[1]\n",
    "#         if response_text.startswith('json'):\n",
    "#             response_text = response_text[4:]\n",
    "#         response_text = response_text.strip()\n",
    "    \n",
    "#     # Parsear JSON\n",
    "#     try:\n",
    "#         user_data = json.loads(response_text)\n",
    "#     except json.JSONDecodeError as e:\n",
    "#         raise ValueError(f\"Error parseando JSON: {e}\\nRespuesta: {response_text}\")\n",
    "    \n",
    "#     # Validaci√≥n b√°sica\n",
    "#     required_fields = USER_PROFILE_SCHEMA['required']\n",
    "#     missing_fields = [f for f in required_fields if f not in user_data or user_data[f] is None]\n",
    "    \n",
    "#     if missing_fields:\n",
    "#         raise ValueError(f\"Faltan campos requeridos: {missing_fields}\")\n",
    "    \n",
    "#     return user_data\n",
    "\n",
    "# # Prueba\n",
    "# test_text = \"\"\"Hola, tengo 45 a√±os, soy mujer. \n",
    "# Mido 1.65 metros y peso 75 kilos. \n",
    "# Mi cintura mide 90 cm.\n",
    "# Duermo unas 6 horas por noche.\n",
    "# Fumo 10 cigarrillos al d√≠a.\n",
    "# Hago ejercicio 2 d√≠as a la semana.\n",
    "# Como 3 porciones de frutas y verduras al d√≠a.\"\"\"\n",
    "\n",
    "# extracted_data = extract_user_data_from_text(test_text)\n",
    "# print(\"\\n‚úÖ DATOS EXTRA√çDOS:\")\n",
    "# print(json.dumps(extracted_data, indent=2, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üß† FASE 7: LLM - COACH CON RAG (H13 a H16)\n",
    "\n",
    "### 7.1 Setup de base de conocimiento local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# # Crear carpeta /kb si no existe\n",
    "# Path('./kb').mkdir(exist_ok=True)\n",
    "\n",
    "# # Crear fichas de conocimiento de ejemplo\n",
    "# KB_CONTENT = {\n",
    "#     'nutricion.md': \"\"\"# Nutrici√≥n Saludable\n",
    "\n",
    "# ## Recomendaciones Generales\n",
    "# - Consumir al menos 5 porciones de frutas y verduras al d√≠a\n",
    "# - Preferir cereales integrales sobre refinados\n",
    "# - Limitar az√∫cares a√±adidos a menos del 10% de calor√≠as totales\n",
    "# - Reducir sodio a menos de 2300 mg/d√≠a\n",
    "\n",
    "# ## Para Prevenci√≥n de Diabetes\n",
    "# - Aumentar fibra diet√©tica (25-30g/d√≠a)\n",
    "# - Elegir alimentos con bajo √≠ndice glic√©mico\n",
    "# - Limitar bebidas azucaradas\n",
    "# - Preferir grasas saludables (omega-3, aceite de oliva)\n",
    "\n",
    "# Fuente: American Diabetes Association, 2024\n",
    "# \"\"\",\n",
    "\n",
    "#     'diabetes_prevention.md': \"\"\"# Prevenci√≥n de Diabetes Tipo 2\n",
    "\n",
    "# ## Factores de Riesgo Modificables\n",
    "# - Sobrepeso u obesidad abdominal\n",
    "# - Sedentarismo y baja masa muscular\n",
    "# - Dieta alta en carbohidratos refinados y baja en fibra\n",
    "# - Sue√±o insuficiente o irregular\n",
    "# - Tabaquismo\n",
    "\n",
    "# ## Estrategias Prioritarias\n",
    "# - **P√©rdida de peso 5-7%** ‚Üí Reduce riesgo en 58% (DPP Study, NEJM 2002)\n",
    "# - **Actividad f√≠sica** ‚Üí 150 min/semana moderada + fuerza 2 d√≠as/semana\n",
    "# - **Patr√≥n alimentario** ‚Üí Bajo √≠ndice glic√©mico, alto en fibra, grasas saludables\n",
    "# - **Monitoreo** ‚Üí A1c anual si IMC ‚â• 25 kg/m¬≤ + factor de riesgo\n",
    "# - **Sue√±o** ‚Üí 7-9 horas constantes, higiene del sue√±o\n",
    "\n",
    "# ## Se√±ales de Alerta para Derivaci√≥n\n",
    "# - Glucosa en ayunas ‚â• 126 mg/dL\n",
    "# - HbA1c ‚â• 6.5%\n",
    "# - P√©rdida de peso no intencionada, poliuria o polidipsia\n",
    "\n",
    "# Fuente: ADA Standards of Care 2024 | CDC National Diabetes Prevention Program\n",
    "# \"\"\",\n",
    "    \n",
    "#     'actividad_fisica.md': \"\"\"# Actividad F√≠sica\n",
    "\n",
    "# ## Recomendaciones OMS\n",
    "# - Adultos: 150-300 min/semana de actividad moderada, o 75-150 min de actividad vigorosa\n",
    "# - Ejercicios de fortalecimiento muscular 2+ d√≠as/semana\n",
    "# - Reducir tiempo sedentario\n",
    "\n",
    "# ## Beneficios para Prevenci√≥n\n",
    "# - Mejora sensibilidad a la insulina\n",
    "# - Ayuda a mantener peso saludable\n",
    "# - Reduce presi√≥n arterial\n",
    "# - Mejora perfil lip√≠dico\n",
    "\n",
    "# ## Inicio Gradual\n",
    "# - Comenzar con 10-15 min/d√≠a\n",
    "# - Aumentar 5 min/semana\n",
    "# - Incorporar actividades placenteras\n",
    "\n",
    "# Fuente: WHO Physical Activity Guidelines, 2020\n",
    "# \"\"\",\n",
    "    \n",
    "#     'sue√±o.md': \"\"\"# Higiene del Sue√±o\n",
    "\n",
    "# ## Duraci√≥n Recomendada\n",
    "# - Adultos: 7-9 horas por noche\n",
    "# - Dormir menos de 7 horas aumenta riesgo cardiometab√≥lico\n",
    "\n",
    "# ## Pr√°cticas Saludables\n",
    "# - Horario regular de sue√±o (incluso fines de semana)\n",
    "# - Evitar pantallas 1 hora antes de dormir\n",
    "# - Limitar cafe√≠na despu√©s de las 14:00\n",
    "# - Ambiente fresco, oscuro y silencioso\n",
    "# - Evitar comidas pesadas 3 horas antes\n",
    "\n",
    "# ## Relaci√≥n con Salud Metab√≥lica\n",
    "# - Sue√±o insuficiente altera hormonas del apetito\n",
    "# - Aumenta resistencia a la insulina\n",
    "# - Eleva presi√≥n arterial\n",
    "\n",
    "# Fuente: National Sleep Foundation, 2023\n",
    "# \"\"\",\n",
    "    \n",
    "#     'tabaquismo.md': \"\"\"# Cesaci√≥n del Tabaquismo\n",
    "\n",
    "# ## Impacto en Salud\n",
    "# - Fumar duplica riesgo de diabetes tipo 2\n",
    "# - Aumenta significativamente riesgo cardiovascular\n",
    "# - Afecta circulaci√≥n y presi√≥n arterial\n",
    "\n",
    "# ## Estrategias para Dejar de Fumar\n",
    "# 1. Fijar fecha de cesaci√≥n\n",
    "# 2. Informar a familiares y amigos\n",
    "# 3. Identificar gatillantes\n",
    "# 4. Considerar terapia de reemplazo nicot√≠nico\n",
    "# 5. Buscar apoyo profesional\n",
    "\n",
    "# ## Beneficios Inmediatos\n",
    "# - 20 min: Presi√≥n arterial y pulso normalizan\n",
    "# - 24 horas: Riesgo de ataque card√≠aco disminuye\n",
    "# - 2 semanas: Mejora circulaci√≥n y funci√≥n pulmonar\n",
    "\n",
    "# Fuente: CDC Smoking Cessation Guidelines, 2024\n",
    "# \"\"\"\n",
    "# }\n",
    "\n",
    "# # Guardar fichas\n",
    "# for filename, content in KB_CONTENT.items():\n",
    "#     with open(f'./kb/{filename}', 'w', encoding='utf-8') as f:\n",
    "#         f.write(content)\n",
    "\n",
    "# print(f\"‚úÖ Base de conocimiento creada: {len(KB_CONTENT)} fichas\")\n",
    "# print(\"   Archivos en ./kb/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 Sistema RAG simple con BM25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# from rank_bm25 import BM25Okapi\n",
    "# import re\n",
    "\n",
    "# class SimpleRAG:\n",
    "#     \"\"\"Sistema RAG b√°sico con BM25 para b√∫squeda en /kb local.\"\"\"\n",
    "    \n",
    "#     def __init__(self, kb_dir='./kb'):\n",
    "#         self.kb_dir = Path(kb_dir)\n",
    "#         self.documents = []\n",
    "#         self.doc_names = []\n",
    "        \n",
    "#         # Cargar todos los .md\n",
    "#         for md_file in self.kb_dir.glob('*.md'):\n",
    "#             with open(md_file, 'r', encoding='utf-8') as f:\n",
    "#                 content = f.read()\n",
    "#                 self.documents.append(content)\n",
    "#                 self.doc_names.append(md_file.name)\n",
    "        \n",
    "#         # Tokenizar para BM25\n",
    "#         self.tokenized_docs = [self._tokenize(doc) for doc in self.documents]\n",
    "#         self.bm25 = BM25Okapi(self.tokenized_docs)\n",
    "        \n",
    "#         print(f\"‚úÖ RAG inicializado con {len(self.documents)} documentos\")\n",
    "    \n",
    "#     def _tokenize(self, text):\n",
    "#         \"\"\"Tokenizaci√≥n simple.\"\"\"\n",
    "#         text = text.lower()\n",
    "#         tokens = re.findall(r'\\b\\w+\\b', text)\n",
    "#         return tokens\n",
    "    \n",
    "#     def search(self, query, top_k=3):\n",
    "#         \"\"\"Busca documentos relevantes.\"\"\"\n",
    "#         query_tokens = self._tokenize(query)\n",
    "#         scores = self.bm25.get_scores(query_tokens)\n",
    "        \n",
    "#         # Top K documentos\n",
    "#         top_indices = np.argsort(scores)[-top_k:][::-1]\n",
    "        \n",
    "#         results = []\n",
    "#         for idx in top_indices:\n",
    "#             if scores[idx] > 0:\n",
    "#                 results.append({\n",
    "#                     'filename': self.doc_names[idx],\n",
    "#                     'content': self.documents[idx],\n",
    "#                     'score': scores[idx]\n",
    "#                 })\n",
    "        \n",
    "#         return results\n",
    "\n",
    "# # Inicializar RAG\n",
    "# rag = SimpleRAG('./kb')\n",
    "\n",
    "# # Prueba\n",
    "# test_query = \"recomendaciones para prevenir diabetes con alimentaci√≥n\"\n",
    "# results = rag.search(test_query, top_k=2)\n",
    "\n",
    "# print(f\"\\nüîç B√∫squeda: '{test_query}'\")\n",
    "# print(f\"   Encontrados: {len(results)} documentos\")\n",
    "# for r in results:\n",
    "#     print(f\"   - {r['filename']} (score: {r['score']:.2f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3 Funci√≥n de Coach con RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# from typing import Optional\n",
    "\n",
    "# PRIORITY_MAP = {\n",
    "#     'peso': 'control de peso',\n",
    "#     'bmi': 'control de peso',\n",
    "#     'waist': 'control de peso',\n",
    "#     'central_obesity': 'control de peso',\n",
    "#     'sleep': 'mejora del sue√±o',\n",
    "#     'sicig': 'cesaci√≥n de tabaquismo',  # placeholder to avoid missing partial matches\n",
    "#     'smoker': 'cesaci√≥n de tabaquismo',\n",
    "#     'cigarette': 'cesaci√≥n de tabaquismo',\n",
    "#     'activity': 'aumento actividad f√≠sica',\n",
    "#     'sedentary': 'aumento actividad f√≠sica',\n",
    "#     'active': 'aumento actividad f√≠sica',\n",
    "#     'lifestyle': 'mejora alimentaci√≥n y h√°bitos',\n",
    "#     'fruit': 'mejora alimentaci√≥n y h√°bitos',\n",
    "#     'diet': 'mejora alimentaci√≥n y h√°bitos'\n",
    "# }\n",
    "\n",
    "# REFERRAL_THRESHOLD = 0.70\n",
    "\n",
    "# def _map_driver_to_priority(feature_name: str) -> Optional[str]:\n",
    "#     lower_name = feature_name.lower()\n",
    "#     for key, area in PRIORITY_MAP.items():\n",
    "#         if key in lower_name:\n",
    "#             return area\n",
    "#     return None\n",
    "\n",
    "# def generate_personalized_plan_with_shap(user_data: dict, risk_score: float, shap_drivers: list, top_k_docs: int = 3) -> dict:\n",
    "#     \"\"\"Genera un plan personalizado usando RAG e informaci√≥n SHAP.\"\"\"\n",
    "#     if rag is None:\n",
    "#         raise RuntimeError(\"El sistema RAG no est√° inicializado. Ejecuta SimpleRAG antes de generar el plan.\")\n",
    "\n",
    "#     shap_drivers = shap_drivers or []\n",
    "\n",
    "#     explanation_lines = []\n",
    "#     priority_areas = []\n",
    "\n",
    "#     for driver in shap_drivers:\n",
    "#         feature = driver.get('feature', 'feature_desconocida')\n",
    "#         value = driver.get('feature_value')\n",
    "#         shap_val = driver.get('shap_value', 0.0)\n",
    "#         impact = 'incrementa' if shap_val > 0 else 'reduce'\n",
    "#         if value is not None and isinstance(value, (int, float)):\n",
    "#             explanation_lines.append(f\"- {feature}: valor {value:.2f} ({impact} riesgo, SHAP {shap_val:+.3f})\")\n",
    "#         else:\n",
    "#             explanation_lines.append(f\"- {feature}: {impact} riesgo (SHAP {shap_val:+.3f})\")\n",
    "\n",
    "#         if shap_val > 0:\n",
    "#             area = _map_driver_to_priority(feature)\n",
    "#             if area:\n",
    "#                 priority_areas.append(area)\n",
    "\n",
    "#     priority_areas = sorted(set(priority_areas))\n",
    "#     if not priority_areas:\n",
    "#         priority_areas = ['mantenimiento de h√°bitos saludables']\n",
    "\n",
    "#     rag_query = \"; \".join(priority_areas)\n",
    "#     relevant_docs = rag.search(rag_query, top_k=top_k_docs)\n",
    "#     if not relevant_docs:\n",
    "#         # Fallback a todo el KB si la b√∫squeda no encuentra resultados\n",
    "#         relevant_docs = [\n",
    "#             {'filename': name, 'content': content, 'score': 0.0}\n",
    "#             for name, content in zip(rag.doc_names, rag.documents)\n",
    "#         ][:top_k_docs]\n",
    "\n",
    "#     context = \"\\n\\n\".join([\n",
    "#         f\"=== {doc['filename']} ===\\n{doc['content']}\"\n",
    "#         for doc in relevant_docs\n",
    "#     ])\n",
    "\n",
    "#     shap_context = \"\\n\".join(explanation_lines) if explanation_lines else \"- Los drivers SHAP no est√°n disponibles, utiliza recomendaciones generales.\"\n",
    "\n",
    "#     risk_level = 'Alto' if risk_score >= 0.6 else 'Moderado' if risk_score >= 0.3 else 'Bajo'\n",
    "#     referral_message = \"\" if risk_score < REFERRAL_THRESHOLD else \"\\n- Riesgo elevado: recomendar evaluaci√≥n m√©dica profesional.\"\n",
    "\n",
    "#     prompt = f\"\"\"Eres un coach de bienestar preventivo especializado en diabetes.\n",
    "# Genera un plan de 2 semanas con acciones SMART priorizando los factores SHAP m√°s cr√≠ticos.\n",
    "\n",
    "# PERFIL DEL USUARIO:\n",
    "# {json.dumps(user_data, indent=2, ensure_ascii=False)}\n",
    "\n",
    "# EVALUACI√ìN DE RIESGO:\n",
    "# - Puntaje de riesgo: {risk_score:.1%}\n",
    "# - Nivel: {risk_level}{referral_message}\n",
    "\n",
    "# AN√ÅLISIS EXPLICABLE (SHAP):\n",
    "# {shap_context}\n",
    "\n",
    "# √ÅREAS PRIORITARIAS:\n",
    "# {', '.join(priority_areas)}\n",
    "\n",
    "# CONOCIMIENTO DISPONIBLE:\n",
    "# {context}\n",
    "\n",
    "# INSTRUCCIONES:\n",
    "# 1. Proporciona un plan de 2 semanas con acciones SMART espec√≠ficas.\n",
    "# 2. Prioriza los factores que m√°s incrementan el riesgo seg√∫n SHAP.\n",
    "# 3. Usa SOLO la informaci√≥n presente en el contexto; cita las fuentes con [archivo.md].\n",
    "# 4. Incluye recordatorio expl√≠cito: \"Este plan NO es un diagn√≥stico m√©dico. Consulta con un profesional de salud.\"\n",
    "# 5. A√±ade recomendaciones de derivaci√≥n si el riesgo supera {REFERRAL_THRESHOLD:.0%}.\n",
    "\n",
    "# FORMATO JSON:\n",
    "# {{\"plan\": \"texto del plan\", \"sources\": [\"archivo1.md\"], \"priority_actions\": [\"acci√≥n1\", \"acci√≥n2\"]}}\n",
    "# \"\"\"\n",
    "\n",
    "#     response = client.chat.completions.create(\n",
    "#         model=\"gpt-4o\",\n",
    "#         max_tokens=2000,\n",
    "#         messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "#         response_format={\"type\": \"json_object\"}\n",
    "#     )\n",
    "\n",
    "#     response_text = response.choices[0].message.content.strip()\n",
    "#     if response_text.startswith('```'):\n",
    "#         response_text = response_text.split('```')[1]\n",
    "#         if response_text.startswith('json'):\n",
    "#             response_text = response_text[4:]\n",
    "#         response_text = response_text.strip()\n",
    "\n",
    "#     plan_data = json.loads(response_text)\n",
    "\n",
    "#     valid_sources = {doc['filename'] for doc in relevant_docs}\n",
    "#     cited_sources = plan_data.get('sources', [])\n",
    "#     for source in cited_sources:\n",
    "#         if source not in valid_sources:\n",
    "#             print(f\"‚ö†Ô∏è Fuente potencialmente alucinada: {source}\")\n",
    "\n",
    "#     plan_data.setdefault('sources', list(valid_sources))\n",
    "#     plan_data.setdefault('priority_actions', [])\n",
    "#     return plan_data\n",
    "\n",
    "\n",
    "# def generate_personalized_plan(user_data: dict, risk_score: float, shap_drivers: list) -> dict:\n",
    "#     \"\"\"Interfaz principal para generaci√≥n de planes con SHAP.\"\"\"\n",
    "#     return generate_personalized_plan_with_shap(user_data, risk_score, shap_drivers)\n",
    "\n",
    "# # Prueba con datos de ejemplo\n",
    "# sample_shap_drivers = [\n",
    "#     {'feature': 'bmi', 'feature_value': 32.4, 'shap_value': 0.185},\n",
    "#     {'feature': 'sleep_hours', 'feature_value': 6.0, 'shap_value': 0.112},\n",
    "#     {'feature': 'cigarettes_per_day', 'feature_value': 10, 'shap_value': 0.095}\n",
    "# ]\n",
    "\n",
    "# sample_user_data = {\n",
    "#     \"age\": 45,\n",
    "#     \"sex\": \"F\",\n",
    "#     \"height_cm\": 165,\n",
    "#     \"weight_kg\": 75,\n",
    "#     \"waist_cm\": 90,\n",
    "#     \"sleep_hours\": 6,\n",
    "#     \"smokes_cig_day\": 10,\n",
    "#     \"days_mvpa_week\": 2,\n",
    "#     \"fruit_veg_portions_day\": 3\n",
    "# }\n",
    "\n",
    "# sample_risk_score = 0.65\n",
    "\n",
    "# # Nota: Esta celda requiere OPENAI_API_KEY configurada para ejecutarse correctamente\n",
    "# # plan = generate_personalized_plan(sample_user_data, sample_risk_score, sample_shap_drivers)\n",
    "# # print(\"\\n‚úÖ PLAN GENERADO:\")\n",
    "# # print(\"\\n\" + plan['plan'])\n",
    "# # print(f\"\\nüìö Fuentes citadas: {', '.join(plan['sources'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üöÄ FASE 8: API FASTAPI (H16 a H18)\n",
    "\n",
    "### 8.1 Crear API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# # %%writefile api_main.py\n",
    "# from fastapi import FastAPI, HTTPException\n",
    "# from pydantic import BaseModel, Field\n",
    "# from typing import List, Optional\n",
    "# import joblib\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import json\n",
    "# from pathlib import Path\n",
    "# import shap\n",
    "\n",
    "# # Inicializar FastAPI\n",
    "# app = FastAPI(\n",
    "#     title=\"Coach de Bienestar Preventivo\",\n",
    "#     description=\"API para estimaci√≥n de riesgo cardiometab√≥lico y coaching personalizado\",\n",
    "#     version=\"1.0.0\"\n",
    "# )\n",
    "\n",
    "# # Cargar modelo y artefactos\n",
    "# model = joblib.load('model_xgboost.pkl')\n",
    "# imputer = joblib.load('imputer.pkl')\n",
    "# feature_names = joblib.load('feature_names.pkl')\n",
    "# explainer = shap.TreeExplainer(model)\n",
    "\n",
    "# FEATURE_DESCRIPTIONS = {\n",
    "#     'age': 'Edad',\n",
    "#     'age_squared': 'Edad al cuadrado',\n",
    "#     'sex_male': 'Sexo masculino',\n",
    "#     'bmi': '√çndice de Masa Corporal',\n",
    "#     'waist_height_ratio': 'Proporci√≥n cintura-altura',\n",
    "#     'high_waist_height_ratio': 'Relaci√≥n cintura-altura elevada',\n",
    "#     'central_obesity': 'Obesidad abdominal',\n",
    "#     'sleep_hours': 'Horas de sue√±o',\n",
    "#     'poor_sleep': 'Sue√±o insuficiente/excesivo',\n",
    "#     'cigarettes_per_day': 'Cigarrillos por d√≠a',\n",
    "#     'current_smoker': 'Fumador activo',\n",
    "#     'ever_smoker': 'Historial de tabaquismo',\n",
    "#     'total_active_days': 'D√≠as activos por semana',\n",
    "#     'meets_activity_guidelines': 'Cumple actividad recomendada',\n",
    "#     'sedentary_flag': 'Indicador de sedentarismo',\n",
    "#     'lifestyle_risk_score': 'Puntaje de riesgo de estilo de vida',\n",
    "#     'bmi_age_interaction': 'Interacci√≥n IMC * edad',\n",
    "#     'waist_age_interaction': 'Interacci√≥n cintura-altura * edad',\n",
    "#     'high_risk_profile': 'Edad ‚â•45 + IMC ‚â•30'\n",
    "# }\n",
    "\n",
    "# REFERRAL_THRESHOLD = 0.70\n",
    "\n",
    "# # Modelos de datos\n",
    "# class UserProfile(BaseModel):\n",
    "#     age: int = Field(..., ge=18, le=85)\n",
    "#     sex: str = Field(..., pattern=\"^[MF]$\")\n",
    "#     height_cm: float = Field(..., ge=120, le=220)\n",
    "#     weight_kg: float = Field(..., ge=30, le=220)\n",
    "#     waist_cm: float = Field(..., ge=40, le=170)\n",
    "#     sleep_hours: Optional[float] = Field(None, ge=3, le=14)\n",
    "#     smokes_cig_day: Optional[int] = Field(None, ge=0, le=60)\n",
    "#     days_mvpa_week: Optional[int] = Field(None, ge=0, le=7)\n",
    "#     fruit_veg_portions_day: Optional[float] = Field(None, ge=0, le=12)\n",
    "\n",
    "# class RiskResponse(BaseModel):\n",
    "#     score: float\n",
    "#     risk_level: str\n",
    "#     drivers: List[dict]\n",
    "#     recommendation: str\n",
    "\n",
    "# class CoachRequest(BaseModel):\n",
    "#     user_profile: UserProfile\n",
    "#     risk_score: float\n",
    "#     top_drivers: List[str]\n",
    "\n",
    "# class CoachResponse(BaseModel):\n",
    "#     plan: str\n",
    "#     sources: List[str]\n",
    "\n",
    "\n",
    "# def build_feature_frame(profile: UserProfile) -> pd.DataFrame:\n",
    "#     bmi = profile.weight_kg / ((profile.height_cm / 100) ** 2)\n",
    "#     waist_height_ratio = profile.waist_cm / profile.height_cm\n",
    "#     sleep_hours = profile.sleep_hours if profile.sleep_hours is not None else 7.5\n",
    "#     cigarettes = profile.smokes_cig_day if profile.smokes_cig_day is not None else 0\n",
    "#     total_active_days = profile.days_mvpa_week if profile.days_mvpa_week is not None else 0\n",
    "\n",
    "#     central_obesity = int(\n",
    "#         (profile.sex == 'M' and profile.waist_cm >= 102) or\n",
    "#         (profile.sex == 'F' and profile.waist_cm >= 88)\n",
    "#     )\n",
    "#     poor_sleep = int(sleep_hours < 7 or sleep_hours > 9)\n",
    "#     current_smoker = int(cigarettes > 0)\n",
    "#     sedentary_flag = int(total_active_days < 5)\n",
    "\n",
    "#     feature_values = {\n",
    "#         'age': profile.age,\n",
    "#         'age_squared': profile.age ** 2,\n",
    "#         'sex_male': 1 if profile.sex == 'M' else 0,\n",
    "#         'bmi': bmi,\n",
    "#         'waist_height_ratio': waist_height_ratio,\n",
    "#         'high_waist_height_ratio': int(waist_height_ratio >= 0.5),\n",
    "#         'central_obesity': central_obesity,\n",
    "#         'sleep_hours': sleep_hours,\n",
    "#         'poor_sleep': poor_sleep,\n",
    "#         'cigarettes_per_day': cigarettes,\n",
    "#         'current_smoker': current_smoker,\n",
    "#         'ever_smoker': current_smoker,\n",
    "#         'total_active_days': total_active_days,\n",
    "#         'meets_activity_guidelines': int(total_active_days >= 5),\n",
    "#         'sedentary_flag': sedentary_flag,\n",
    "#         'lifestyle_risk_score': poor_sleep + current_smoker + sedentary_flag,\n",
    "#         'bmi_age_interaction': bmi * profile.age,\n",
    "#         'waist_age_interaction': waist_height_ratio * profile.age,\n",
    "#         'high_risk_profile': int(bmi >= 30 and profile.age >= 45)\n",
    "#     }\n",
    "\n",
    "#     features_df = pd.DataFrame([feature_values])\n",
    "#     for feat in feature_names:\n",
    "#         if feat not in features_df.columns:\n",
    "#             features_df[feat] = 0\n",
    "#     return features_df[feature_names]\n",
    "\n",
    "\n",
    "# # Endpoints\n",
    "# @app.get(\"/\")\n",
    "# def read_root():\n",
    "#     return {\n",
    "#         \"message\": \"Coach de Bienestar Preventivo API\",\n",
    "#         \"version\": \"1.0.0\",\n",
    "#         \"endpoints\": [\"/predict\", \"/coach\", \"/health\"]\n",
    "#     }\n",
    "\n",
    "# @app.get(\"/health\")\n",
    "# def health_check():\n",
    "#     return {\"status\": \"healthy\", \"model_loaded\": model is not None}\n",
    "\n",
    "# @app.post(\"/predict\", response_model=RiskResponse)\n",
    "# def predict_risk(profile: UserProfile):\n",
    "#     \"\"\"Endpoint de predicci√≥n de riesgo cardiometab√≥lico con explicabilidad SHAP.\"\"\"\n",
    "#     try:\n",
    "#         X = build_feature_frame(profile)\n",
    "#         X_imp = imputer.transform(X)\n",
    "#         X_imp_df = pd.DataFrame(X_imp, columns=feature_names)\n",
    "\n",
    "#         risk_score = float(model.predict_proba(X_imp)[0, 1])\n",
    "\n",
    "#         if risk_score < 0.3:\n",
    "#             risk_level = \"Bajo\"\n",
    "#             recommendation = \"Mantener h√°bitos saludables\"\n",
    "#         elif risk_score < 0.6:\n",
    "#             risk_level = \"Moderado\"\n",
    "#             recommendation = \"Mejorar estilo de vida con coaching personalizado\"\n",
    "#         else:\n",
    "#             risk_level = \"Alto\"\n",
    "#             recommendation = \"Consultar con profesional de salud urgentemente\"\n",
    "#             if risk_score >= REFERRAL_THRESHOLD:\n",
    "#                 recommendation += \" y coordinar evaluaci√≥n m√©dica profesional\"\n",
    "\n",
    "#         shap_values = explainer.shap_values(X_imp_df)\n",
    "#         if isinstance(shap_values, list):\n",
    "#             shap_values = shap_values[1] if len(shap_values) > 1 else shap_values[0]\n",
    "#         row_shap = shap_values[0]\n",
    "#         top_indices = np.argsort(np.abs(row_shap))[-5:][::-1]\n",
    "\n",
    "#         drivers = []\n",
    "#         for idx in top_indices:\n",
    "#             feature = feature_names[idx]\n",
    "#             shap_val = float(row_shap[idx])\n",
    "#             impact = 'aumenta' if shap_val > 0 else 'reduce'\n",
    "#             drivers.append({\n",
    "#                 \"feature\": feature,\n",
    "#                 \"description\": FEATURE_DESCRIPTIONS.get(feature, feature),\n",
    "#                 \"value\": float(X_imp_df.iloc[0, idx]),\n",
    "#                 \"shap_value\": shap_val,\n",
    "#                 \"impact\": impact\n",
    "#             })\n",
    "\n",
    "#         return RiskResponse(\n",
    "#             score=risk_score,\n",
    "#             risk_level=risk_level,\n",
    "#             drivers=drivers,\n",
    "#             recommendation=recommendation\n",
    "#         )\n",
    "\n",
    "#     except Exception as e:\n",
    "#         raise HTTPException(status_code=500, detail=str(e))\n",
    "\n",
    "# @app.post(\"/coach\", response_model=CoachResponse)\n",
    "# def generate_coach_plan(request: CoachRequest):\n",
    "#     \"\"\"\n",
    "#     Endpoint de generaci√≥n de plan personalizado con RAG.\n",
    "    \n",
    "#     NOTA: Requiere integraci√≥n con funci√≥n generate_personalized_plan()\n",
    "#     \"\"\"\n",
    "#     try:\n",
    "#         plan_text = f\"\"\"Plan personalizado de 2 semanas para mejorar tu salud.\n",
    "        \n",
    "# Basado en tu perfil (edad {request.user_profile.age}, riesgo {request.risk_score:.1%}),\n",
    "# te recomendamos enfocarte en: {', '.join(request.top_drivers)}.\n",
    "\n",
    "# DISCLAIMER: Este plan NO es un diagn√≥stico m√©dico. Consulta con un profesional de salud.\"\"\"\n",
    "\n",
    "#         return CoachResponse(\n",
    "#             plan=plan_text,\n",
    "#             sources=[\"nutricion.md\", \"actividad_fisica.md\"]\n",
    "#         )\n",
    "\n",
    "#     except Exception as e:\n",
    "#         raise HTTPException(status_code=500, detail=str(e))\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     import uvicorn\n",
    "#     uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n",
    "\n",
    "# print(\"‚úÖ API guardada en api_main.py\")\n",
    "# print(\"   Para ejecutar: uvicorn api_main:app --reload\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üé® FASE 9: APP STREAMLIT (H18 a H22)\n",
    "\n",
    "### 9.1 Crear interfaz de usuario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# # %%writefile app_streamlit.py\n",
    "# import streamlit as st\n",
    "# import requests\n",
    "# import json\n",
    "# from fpdf import FPDF\n",
    "# import base64\n",
    "\n",
    "# # Configuraci√≥n de p√°gina\n",
    "# st.set_page_config(\n",
    "#     page_title=\"Coach de Bienestar Preventivo\",\n",
    "#     page_icon=\"üè•\",\n",
    "#     layout=\"wide\"\n",
    "# )\n",
    "\n",
    "# # URL de la API (ajustar seg√∫n deployment)\n",
    "# API_URL = \"http://localhost:8000\"\n",
    "\n",
    "# # Header\n",
    "# st.title(\"üè• Coach de Bienestar Preventivo\")\n",
    "# st.markdown(\"\"\"\n",
    "# Este sistema estima tu riesgo cardiometab√≥lico y genera un plan personalizado.\n",
    "\n",
    "# **‚ö†Ô∏è DISCLAIMER:** Este NO es un diagn√≥stico m√©dico. Consulta con un profesional de salud.\n",
    "# \"\"\")\n",
    "\n",
    "# # Sidebar para formulario\n",
    "# with st.sidebar:\n",
    "#     st.header(\"üìã Tu Perfil\")\n",
    "    \n",
    "#     # Datos demogr√°ficos\n",
    "#     st.subheader(\"Demogr√°fico\")\n",
    "#     age = st.number_input(\"Edad\", min_value=18, max_value=85, value=45)\n",
    "#     sex = st.selectbox(\"Sexo\", [\"M\", \"F\"], format_func=lambda x: \"Masculino\" if x == \"M\" else \"Femenino\")\n",
    "    \n",
    "#     # Antropometr√≠a\n",
    "#     st.subheader(\"Antropometr√≠a\")\n",
    "#     height_cm = st.number_input(\"Altura (cm)\", min_value=120, max_value=220, value=170)\n",
    "#     weight_kg = st.number_input(\"Peso (kg)\", min_value=30, max_value=220, value=75)\n",
    "#     waist_cm = st.number_input(\"Cintura (cm)\", min_value=40, max_value=170, value=90)\n",
    "    \n",
    "#     # Calcular IMC\n",
    "#     bmi = weight_kg / ((height_cm / 100) ** 2)\n",
    "#     st.info(f\"IMC: {bmi:.1f}\")\n",
    "    \n",
    "#     # Estilo de vida\n",
    "#     st.subheader(\"Estilo de Vida\")\n",
    "#     sleep_hours = st.slider(\"Horas de sue√±o/d√≠a\", 3, 12, 7)\n",
    "#     smokes_cig_day = st.number_input(\"Cigarrillos/d√≠a\", min_value=0, max_value=60, value=0)\n",
    "#     days_mvpa_week = st.slider(\"D√≠as de ejercicio/semana\", 0, 7, 3)\n",
    "#     fruit_veg_portions_day = st.slider(\"Porciones frutas/verduras/d√≠a\", 0, 12, 5)\n",
    "    \n",
    "#     # Bot√≥n de evaluaci√≥n\n",
    "#     evaluate_button = st.button(\"üîç Evaluar Riesgo\", type=\"primary\")\n",
    "\n",
    "# # Main area\n",
    "# if evaluate_button:\n",
    "#     # Preparar datos\n",
    "#     user_data = {\n",
    "#         \"age\": age,\n",
    "#         \"sex\": sex,\n",
    "#         \"height_cm\": height_cm,\n",
    "#         \"weight_kg\": weight_kg,\n",
    "#         \"waist_cm\": waist_cm,\n",
    "#         \"sleep_hours\": sleep_hours,\n",
    "#         \"smokes_cig_day\": smokes_cig_day,\n",
    "#         \"days_mvpa_week\": days_mvpa_week,\n",
    "#         \"fruit_veg_portions_day\": fruit_veg_portions_day\n",
    "#     }\n",
    "    \n",
    "#     # Llamar a API de predicci√≥n\n",
    "#     with st.spinner(\"Analizando tu perfil...\"):\n",
    "#         try:\n",
    "#             response = requests.post(f\"{API_URL}/predict\", json=user_data)\n",
    "            \n",
    "#             if response.status_code == 200:\n",
    "#                 result = response.json()\n",
    "                \n",
    "#                 # Mostrar resultado\n",
    "#                 col1, col2, col3 = st.columns(3)\n",
    "                \n",
    "#                 with col1:\n",
    "#                     risk_score = result['score']\n",
    "#                     st.metric(\n",
    "#                         \"Puntaje de Riesgo\",\n",
    "#                         f\"{risk_score:.1%}\",\n",
    "#                         delta=None\n",
    "#                     )\n",
    "                \n",
    "#                 with col2:\n",
    "#                     st.metric(\n",
    "#                         \"Nivel de Riesgo\",\n",
    "#                         result['risk_level']\n",
    "#                     )\n",
    "                \n",
    "#                 with col3:\n",
    "#                     # Color seg√∫n riesgo\n",
    "#                     if risk_score < 0.3:\n",
    "#                         color = \"üü¢\"\n",
    "#                     elif risk_score < 0.6:\n",
    "#                         color = \"üü°\"\n",
    "#                     else:\n",
    "#                         color = \"üî¥\"\n",
    "#                     st.metric(\"Indicador\", color)\n",
    "                \n",
    "#                 # Recomendaci√≥n principal\n",
    "#                 st.info(f\"üìå {result['recommendation']}\")\n",
    "                \n",
    "#                 # Drivers de riesgo\n",
    "#                 st.subheader(\"üéØ Principales Factores de Riesgo\")\n",
    "                \n",
    "#                 drivers_df = pd.DataFrame(result['drivers'])\n",
    "#                 st.dataframe(drivers_df, use_container_width=True)\n",
    "                \n",
    "#                 # Generar plan personalizado\n",
    "#                 if st.button(\"üìù Generar Plan Personalizado\"):\n",
    "#                     with st.spinner(\"Creando tu plan...\"):\n",
    "#                         coach_request = {\n",
    "#                             \"user_profile\": user_data,\n",
    "#                             \"risk_score\": risk_score,\n",
    "#                             \"top_drivers\": [d['feature'] for d in result['drivers'][:3]]\n",
    "#                         }\n",
    "                        \n",
    "#                         coach_response = requests.post(f\"{API_URL}/coach\", json=coach_request)\n",
    "                        \n",
    "#                         if coach_response.status_code == 200:\n",
    "#                             plan_data = coach_response.json()\n",
    "                            \n",
    "#                             st.subheader(\"üìã Tu Plan de Bienestar Personalizado\")\n",
    "#                             st.markdown(plan_data['plan'])\n",
    "                            \n",
    "#                             st.caption(f\"üìö Fuentes: {', '.join(plan_data['sources'])}\")\n",
    "                            \n",
    "#                             # Bot√≥n de descarga PDF\n",
    "#                             if st.button(\"‚¨áÔ∏è Descargar PDF\"):\n",
    "#                                 st.success(\"PDF generado! (implementar funci√≥n de generaci√≥n)\")\n",
    "#                         else:\n",
    "#                             st.error(f\"Error generando plan: {coach_response.status_code}\")\n",
    "#             else:\n",
    "#                 st.error(f\"Error en predicci√≥n: {response.status_code}\")\n",
    "                \n",
    "#         except Exception as e:\n",
    "#             st.error(f\"Error conectando con la API: {e}\")\n",
    "#             st.info(\"Aseg√∫rate de que la API est√© corriendo en http://localhost:8000\")\n",
    "\n",
    "# # Footer\n",
    "# st.markdown(\"---\")\n",
    "# st.caption(\"\"\"\n",
    "# Desarrollado para Hackathon IA Duoc UC 2025 | \n",
    "# Basado en datos NHANES | \n",
    "# ‚ö†Ô∏è No sustituye atenci√≥n m√©dica profesional\n",
    "# \"\"\")\n",
    "\n",
    "# print(\"‚úÖ App Streamlit guardada en app_streamlit.py\")\n",
    "# print(\"   Para ejecutar: streamlit run app_streamlit.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üì¶ FASE 10: DEPLOYMENT Y CHECKLIST FINAL (H22 a H27)\n",
    "\n",
    "### 10.1 Preparar para Hugging Face Spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "%%writefile requirements.txt\n",
    "pandas==2.0.3\n",
    "numpy==1.24.3\n",
    "scikit-learn==1.3.0\n",
    "xgboost==2.0.0\n",
    "lightgbm==4.0.0\n",
    "fastapi==0.104.0\n",
    "uvicorn==0.24.0\n",
    "pydantic==2.4.2\n",
    "streamlit==1.28.0\n",
    "openai==1.12.0\n",
    "rank-bm25==0.2.2\n",
    "matplotlib==3.7.2\n",
    "seaborn==0.12.2\n",
    "plotly==5.17.0\n",
    "shap==0.43.0\n",
    "joblib==1.3.2\n",
    "fpdf==1.7.2\n",
    "requests==2.31.0\n",
    "\n",
    "print(\"‚úÖ requirements.txt creado\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# %%writefile README.md\n",
    "# # üè• Coach de Bienestar Preventivo con IA H√≠brida\n",
    "\n",
    "# Sistema de estimaci√≥n de riesgo cardiometab√≥lico y coaching personalizado.\n",
    "\n",
    "# ## üéØ Caracter√≠sticas\n",
    "\n",
    "# - **Predicci√≥n de Riesgo**: Modelo XGBoost con AUROC ‚â• 0.80\n",
    "# - **Validaci√≥n Temporal**: Split por ciclos NHANES (2007-2018)\n",
    "# - **Explicabilidad**: Identificaci√≥n de drivers de riesgo\n",
    "# - **Coaching con RAG**: Recomendaciones basadas en evidencia\n",
    "# - **Equidad**: An√°lisis de fairness por subgrupos\n",
    "# - **API REST**: FastAPI con endpoints /predict y /coach\n",
    "# - **App Interactiva**: Streamlit desplegable en HF Spaces\n",
    "\n",
    "# ## üöÄ Quick Start\n",
    "\n",
    "# ```bash\n",
    "# # Instalar dependencias\n",
    "# pip install -r requirements.txt\n",
    "\n",
    "# # Ejecutar API\n",
    "# uvicorn api_main:app --reload\n",
    "\n",
    "# # Ejecutar App (en otra terminal)\n",
    "# streamlit run app_streamlit.py\n",
    "# ```\n",
    "\n",
    "# ## üìä M√©tricas del Modelo\n",
    "\n",
    "# - AUROC: 0.82\n",
    "# - AUPRC: 0.45\n",
    "# - Brier Score: 0.11\n",
    "# - Fairness Gap: 0.03\n",
    "\n",
    "# ## üìÅ Estructura del Proyecto\n",
    "\n",
    "# ```\n",
    "# salud-hackathon-nhanes/\n",
    "# ‚îú‚îÄ‚îÄ data/                  # Datos NHANES\n",
    "# ‚îú‚îÄ‚îÄ kb/                    # Base de conocimiento\n",
    "# ‚îú‚îÄ‚îÄ models/                # Modelos entrenados\n",
    "# ‚îú‚îÄ‚îÄ api_main.py           # FastAPI\n",
    "# ‚îú‚îÄ‚îÄ app_streamlit.py      # Streamlit app\n",
    "# ‚îú‚îÄ‚îÄ requirements.txt       # Dependencias\n",
    "# ‚îî‚îÄ‚îÄ README.md             # Este archivo\n",
    "# ```\n",
    "\n",
    "# ## ‚ö†Ô∏è Disclaimer\n",
    "\n",
    "# Este sistema NO realiza diagn√≥sticos m√©dicos. Siempre consulta con un profesional de salud.\n",
    "\n",
    "# ## üë• Equipo\n",
    "\n",
    "# [Tu equipo aqu√≠]\n",
    "\n",
    "# ## üìÑ Licencia\n",
    "\n",
    "# MIT License\n",
    "\n",
    "# print(\"‚úÖ README.md creado\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.2 Checklist final de entregables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# def print_final_checklist():\n",
    "#     \"\"\"Imprime checklist de entregables del hackathon.\"\"\"\n",
    "    \n",
    "#     checklist = {\n",
    "#         \"1. Repositorio GitHub\": [\n",
    "#             \"[ ] README.md completo\",\n",
    "#             \"[ ] requirements.txt\",\n",
    "#             \"[ ] C√≥digo organizado en carpetas\",\n",
    "#             \"[ ] .gitignore configurado\"\n",
    "#         ],\n",
    "#         \"2. Modelo ML\": [\n",
    "#             \"[ ] AUROC ‚â• 0.80 en test\",\n",
    "#             \"[ ] Brier Score ‚â§ 0.12\",\n",
    "#             \"[ ] Validaci√≥n temporal implementada\",\n",
    "#             \"[ ] Sin fuga de datos verificado\",\n",
    "#             \"[ ] Curvas de calibraci√≥n guardadas\"\n",
    "#         ],\n",
    "#         \"3. Fairness\": [\n",
    "#             \"[ ] M√©tricas por sexo\",\n",
    "#             \"[ ] M√©tricas por edad\",\n",
    "#             \"[ ] M√©tricas por etnia\",\n",
    "#             \"[ ] Gap absoluto < 0.05\",\n",
    "#             \"[ ] An√°lisis guardado en CSV\"\n",
    "#         ],\n",
    "#         \"4. LLM - Extractor\": [\n",
    "#             \"[ ] Validaci√≥n de JSON 100%\",\n",
    "#             \"[ ] Conversi√≥n de unidades\",\n",
    "#             \"[ ] Manejo de errores\"\n",
    "#         ],\n",
    "#         \"5. LLM - Coach con RAG\": [\n",
    "#             \"[ ] Base de conocimiento en /kb\",\n",
    "#             \"[ ] Todas las recomendaciones con fuentes\",\n",
    "#             \"[ ] Sin alucinaciones verificado\",\n",
    "#             \"[ ] Disclaimer visible\"\n",
    "#         ],\n",
    "#         \"6. API FastAPI\": [\n",
    "#             \"[ ] Endpoint /predict funcional\",\n",
    "#             \"[ ] Endpoint /coach funcional\",\n",
    "#             \"[ ] Documentaci√≥n autom√°tica\",\n",
    "#             \"[ ] Manejo de errores\"\n",
    "#         ],\n",
    "#         \"7. App Demo\": [\n",
    "#             \"[ ] Streamlit/Gradio funcional\",\n",
    "#             \"[ ] Formulario completo\",\n",
    "#             \"[ ] Visualizaci√≥n de riesgo\",\n",
    "#             \"[ ] Generaci√≥n de plan\",\n",
    "#             \"[ ] Deploy en HF Spaces\"\n",
    "#         ],\n",
    "#         \"8. Exportaci√≥n\": [\n",
    "#             \"[ ] PDF descargable\",\n",
    "#             \"[ ] Link compartible\"\n",
    "#         ],\n",
    "#         \"9. Documentaci√≥n\": [\n",
    "#             \"[ ] Reporte t√©cnico 2-3 p√°ginas\",\n",
    "#             \"[ ] Bit√°cora de prompts\",\n",
    "#             \"[ ] Descripci√≥n de guardrails\"\n",
    "#         ],\n",
    "#         \"10. Presentaci√≥n\": [\n",
    "#             \"[ ] Slides preparadas\",\n",
    "#             \"[ ] Demo ensayada\",\n",
    "#             \"[ ] Timing 10 min\",\n",
    "#             \"[ ] Backup screenshots\"\n",
    "#         ]\n",
    "#     }\n",
    "    \n",
    "#     print(\"\\n\" + \"=\"*70)\n",
    "#     print(\"üéØ CHECKLIST FINAL - HACKATHON SALUD NHANES\")\n",
    "#     print(\"=\"*70)\n",
    "    \n",
    "#     for category, items in checklist.items():\n",
    "#         print(f\"\\n{category}\")\n",
    "#         for item in items:\n",
    "#             print(f\"  {item}\")\n",
    "    \n",
    "#     print(\"\\n\" + \"=\"*70)\n",
    "#     print(\"üìä R√öBRICA DE PUNTUACI√ìN:\")\n",
    "#     print(\"=\"*70)\n",
    "    \n",
    "#     print(\"\\nüî¨ Rigor t√©cnico ML (30 pts):\")\n",
    "#     print(\"  ‚Ä¢ AUROC ‚â• 0.80: 12 pts - Usa XGBoost con early stopping\")\n",
    "#     print(\"  ‚Ä¢ Brier ‚â§ 0.12: 6 pts - Calibra con isotonic regression\")\n",
    "#     print(\"  ‚Ä¢ Validaci√≥n temporal sin fuga: 6 pts - Split por ciclos, NO k-fold\")\n",
    "#     print(\"  ‚Ä¢ Explicabilidad: 6 pts - SHAP values o feature importance\")\n",
    "    \n",
    "#     print(\"\\nüß† LLMs y RAG (25 pts):\")\n",
    "#     print(\"  ‚Ä¢ Extractor 100% v√°lido: 8 pts - Schema validation + unit conversion\")\n",
    "#     print(\"  ‚Ä¢ Coach con citas: 9 pts - RAG con BM25, validar sources\")\n",
    "#     print(\"  ‚Ä¢ Guardrails: 8 pts - Umbrales + disclaimer + derivaci√≥n\")\n",
    "    \n",
    "#     print(\"\\nüé® Producto y UX (25 pts):\")\n",
    "#     print(\"  ‚Ä¢ App funcional: 10 pts - Streamlit con manejo de errores\")\n",
    "#     print(\"  ‚Ä¢ Export PDF: 5 pts - fpdf o reportlab\")\n",
    "#     print(\"  ‚Ä¢ Claridad: 10 pts - Mensajes simples + UX intuitiva\")\n",
    "    \n",
    "#     print(\"\\nüì¶ Reproducibilidad (15 pts):\")\n",
    "#     print(\"  ‚Ä¢ Repo limpio: 6 pts - requirements.txt + seeds + scripts\")\n",
    "#     print(\"  ‚Ä¢ Documentaci√≥n: 5 pts - README + comentarios\")\n",
    "#     print(\"  ‚Ä¢ Fairness: 4 pts - An√°lisis completo por subgrupos\")\n",
    "    \n",
    "#     print(\"\\nüé§ Presentaci√≥n (15 pts):\")\n",
    "#     print(\"  ‚Ä¢ Storytelling: 6 pts - Hook + problema + impacto\")\n",
    "#     print(\"  ‚Ä¢ Comunicaci√≥n t√©cnica: 5 pts - Explicar sin jerga\")\n",
    "#     print(\"  ‚Ä¢ Timing: 4 pts - 10 min exactos + demo fluida\")\n",
    "    \n",
    "#     print(\"\\n\" + \"=\"*70)\n",
    "#     print(\"üéØ TOTAL POSIBLE: 110 puntos\")\n",
    "#     print(\"\\n¬°√âXITO EN EL HACKATHON! üöÄ\")\n",
    "#     print(\"=\"*70)\n",
    "\n",
    "# print_final_checklist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üéì RECURSOS ADICIONALES\n",
    "\n",
    "### Enlaces √∫tiles:\n",
    "\n",
    "**NHANES:**\n",
    "- [NHANES Website](https://www.cdc.gov/nchs/nhanes/index.htm)\n",
    "- [NHANES Tutorials](https://wwwn.cdc.gov/nchs/nhanes/tutorials/default.aspx)\n",
    "- [Variable Search](https://wwwn.cdc.gov/nchs/nhanes/search/default.aspx)\n",
    "\n",
    "**Machine Learning:**\n",
    "- [XGBoost Documentation](https://xgboost.readthedocs.io/)\n",
    "- [Scikit-learn User Guide](https://scikit-learn.org/stable/user_guide.html)\n",
    "- [Calibration Guide](https://scikit-learn.org/stable/modules/calibration.html)\n",
    "\n",
    "**LLM y RAG:**\n",
    "- [OpenAI API](https://platform.openai.com/docs/)\n",
    "- [RAG Best Practices](https://platform.openai.com/docs/guides/structured-outputs)\n",
    "\n",
    "**Deployment:**\n",
    "- [FastAPI Tutorial](https://fastapi.tiangolo.com/tutorial/)\n",
    "- [Streamlit Docs](https://docs.streamlit.io/)\n",
    "- [Hugging Face Spaces](https://huggingface.co/docs/hub/spaces)\n",
    "\n",
    "### Tips finales:\n",
    "\n",
    "1. **Divisi√≥n de trabajo:** Asignar roles desde H0 (ML, LLM, Frontend, Doc)\n",
    "2. **Priorizaci√≥n:** Asegurar entregables obligatorios primero\n",
    "3. **Testing continuo:** Validar cada componente antes de integrar\n",
    "4. **Backup:** Guardar versiones de modelos y c√≥digo frecuentemente\n",
    "5. **Documentaci√≥n:** Escribir README y reportes en paralelo\n",
    "\n",
    "### Contactos de ayuda:\n",
    "- Data Team: [contacto]\n",
    "- Mentores t√©cnicos: [contactos]\n",
    "- Soporte API: [contacto]\n",
    "\n",
    "---\n",
    "**¬°MUCHO √âXITO EN EL HACKATHON! üöÄ**\n",
    "\n",
    "Recuerda: El objetivo es aprender y crear impacto en salud preventiva. \n",
    "No te preocupes por la perfecci√≥n, enf√≥cate en construir algo funcional y bien fundamentado.\n",
    "\n",
    "**#IAparaSalud #DuocUC2025**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
